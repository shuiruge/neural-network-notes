<TeXmacs|2.1>

<style|book>

<\body>
  <chapter|Scaling and Power-Law>

  <section|Complexity>

  In this section, we introduce the complexity of a dataset examined by a
  model family. It is a preparation for the discussion of power-law between
  the minimum of loss function and the model size.

  <subsection|Distribution Is a Data Generator>

  Ideally, a dataset is described by a distribution. This distribution is in
  fact a data generator. For example, Internet is a generator of texture
  data. Expression <math|x\<sim\>P<rsub|<text|Internet>>> means the text
  <math|x> is generated by Internet. We cannot say how many texture data are
  there on the Internet, since Internet can generate a new one at any time.
  Thus, <math|P<rsub|<text|Internet>>> is really a data generator; and we can
  generate infinite data from <math|P<rsub|<text|Internet>>>, theoretically.

  Since the minimum of loss function depends on both data size and model
  size, by regarding dataset as a distribution instead of a set, we can
  simplify our discussion, regardless of the data size, and focusing on the
  model size only.

  <subsection|Model Family>

  We are to investigate how the minimum of loss function changes with the
  model size (or the number of parameters). Thus, we shall consider a series
  of models with different number of parameters, that is, a model family.
  Explicitly, let <math|f<rsup|<around*|(|n|)>><around*|(|x;\<theta\>|)>>
  denotes a model with <math|n> parameters, where <math|x> is the model input
  and <math|\<theta\>> the parameters, thus
  <math|\<theta\>\<in\>\<bbb-R\><rsup|n>>.<\footnote>
    Ideally, we shall use <math|\<theta\><rsup|<around*|(|n|)>>> for
    indicating <math|\<theta\>> is <math|n>-dimensional. But whenever it
    appears with a <math|f<rsup|<around*|(|n|)>>>, we can figure out its
    dimension. Thus, we omit the superscript of
    <math|\<theta\><rsup|<around*|(|n|)>>>, just writing <math|\<theta\>>.
  </footnote> A model family is a set <math|<around*|{|f<rsup|<around*|(|n|)>><around*|(|x;\<theta\>|)>\|n=1,2,\<ldots\>|}>>.
  Thus for each <math|n>, there is one and only one model in a model family.

  Without losing generality, suppose that
  <math|f<rsup|<around*|(|n|)>><around*|(|x;0|)>=0> for
  <math|\<forall\>x\<in\>\<bbb-R\>>. Otherwise, we can re-define
  <math|f<rsup|<around*|(|n|)>><around*|(|x;\<theta\>|)>> by
  <math|f<rsup|<around*|(|n|)>><around*|(|x;\<theta\>|)>-f<rsup|<around*|(|n|)>><around*|(|x;0|)>>.
  This help introduce <math|f<rsup|<around*|(|0|)>><around*|(|x;\<theta\>|)>>,
  a model without parameter. It is always vanishing:
  <math|f<rsup|<around*|(|0|)>><around*|(|x;\<theta\>|)>\<equiv\>f<rsup|<around*|(|0|)>><around*|(|x;0|)>=0>
  for all <math|x>. So, model family comes to be a set
  <math|<around*|{|f<rsup|<around*|(|n|)>><around*|(|x;\<theta\>|)>\|n\<in\>\<bbb-N\>,\<theta\>\<in\>\<bbb-R\><rsup|n>|}>>
  with <math|f<rsup|<around*|(|n|)>><around*|(|x;0|)>=0> for all <math|x> and
  <math|n>.

  <subsection|Complexity of Dataset Examined by Model Family>

  Let <math|P> a distribution of dataset and <math|f<rsup|<around*|(|n|)>>> a
  model in a model family. The loss function of a supervised learning task is

  <\equation>
    L<rsup|<around*|(|n|)>><around*|(|\<theta\>|)>=<big|int>\<mathd\>x\<mathd\>y
    \ p<around*|(|x,y|)>d<around*|(|y,f<rsup|<around*|(|n|)>><around*|(|x;\<theta\>|)>|)>,
  </equation>

  where <math|d> is some distance between target and model prediction (for
  regression task, it is the mean-squared error; and for classification task,
  it is the relative entropy).

  Recall that <math|f<rsup|<around*|(|0|)>><around*|(|x;\<theta\>|)>> is
  independent of <math|\<theta\>>, so is the
  <math|L<rsup|<around*|(|0|)>><around*|(|\<theta\>|)>>. Thus, we have a
  constant

  <\equation>
    L<rsub|0>\<assign\>L<rsup|<around*|(|0|)>><around*|(|\<theta\>|)>\<equiv\><big|int>\<mathd\>x\<mathd\>y
    \ p<around*|(|x,y|)>d<around*|(|y,0|)>.
  </equation>

  For <math|n\<gtr\>0>, we have <math|\<theta\><rsub|\<star\>>\<assign\>argmin
  L<rsup|<around*|(|n|)>>> and <math|L<rsup|<around*|(|n|)>><rsub|\<star\>>\<assign\>L<rsup|<around*|(|n|)>><around*|(|\<theta\><rsub|\<star\>>|)>>.
  The <math|L<rsup|<around*|(|n|)>><rsub|\<star\>>> has the dimension
  <math|<around*|[|L|]>>. The dimensionless quantity we are to consider shall
  be <math|L<rsup|<around*|(|n|)>><rsub|\<star\>>/L<rsub|0>>.

  Given a model family, the relation between the number of parameters
  <math|n> and the (relative) minimum of loss function
  <math|L<rsub|\<star\>>/L<rsub|0>> reflects the complexity of dataset
  <math|P>. With a fixed <math|L<rsub|\<star\>>/L<rsub|0>> as the target of
  optimization, the more model parameters we use for approaching the
  <math|L<rsub|\<star\>>/L<rsub|0>>, the more complicated the <math|P> should
  be. Or with a fixed <math|n> as the model size, a simpler dataset <math|P>
  will furnish a smaller best-fit loss <math|L<rsub|\<star\>>/L<rsub|0>>. So,
  we call this relation between <math|n> and
  <math|L<rsub|\<star\>>/L<rsub|0>> the <with|font-series|bold|complexity> of
  dataset examined by the model family. The model family can be viewed as a
  ruler that examines the complexity of different dataset. Of course, this
  complexity depends on the choice of the model family. When we change the
  ruler, the result may be changed too.

  <section|Relation between Minimum of Loss and Number of Parameters: A
  Simple Example>

  We are to investigate how the <math|L<rsup|<around*|(|n|)>><rsub|\<star\>>/L<rsub|0>>
  changes with <math|n>. To do so, we first introduce a process that can
  effectively extend the model size. Then, with rescaling, the expression of
  loss function for the enlarged model can be \Pstandardized\Q to be the same
  as that of the original model. This is a recursive process, from which we
  can get some hints on how the <math|L<rsup|<around*|(|n|)>><rsub|\<star\>>/L<rsub|0>>
  changes with <math|n>.

  <subsection|An Extending-Rescaling Process of Loss Function>

  For simplicity, let us consider an one-dimensional regression task. Let
  <math|P> a distribution of dataset, and <math|f<rsup|<around*|(|n|)>>> a
  model in a model family <math|F>, the loss function comes to be

  <\equation>
    L<rsup|<around*|(|n|)>><around*|(|\<theta\>|)>=<big|int>\<mathd\>x\<mathd\>y
    \ p<around*|(|x,y|)><around*|(|f<rsup|<around*|(|n|)>><around*|(|x;\<theta\>|)>-y|)><rsup|2>.
  </equation>

  \;

  We wonder how the <math|L<rsup|<around*|(|n|)>><rsub|\<star\>>/L<rsub|0>>
  changes with <math|n\<rightarrow\>2n>. The point is first train a baseline
  model with <math|f<rsup|<around*|(|n|)>><around*|(|x;\<theta\>|)>>, which
  furnishes a residual error. Then, enlarge the
  <math|f<rsup|<around*|(|n|)>>> to a model with <math|2n> parameters by
  combining with another <math|<wide|f|~><rsup|<around*|(|n|)>>> which fits
  the residual error. We then estimate the minimum of loss for the combined
  model.

  Explicitly, we enlarge the space of parameters by introducing a new model
  <math|<wide|f|~><rsup|<around*|(|n|)>>\<in\>F> and replacing
  <math|f<rsup|<around*|(|n|)>><around*|(|x;\<theta\>|)>\<rightarrow\>f<rsup|<around*|(|n|)>><around*|(|x;\<theta\>|)>+r<rsup|<around*|(|n|)>>
  <wide|f|~><rsup|<around*|(|n|)>><around*|(|x;<wide|\<theta\>|~>|)>>, where
  <math|r<rsup|<around*|(|n|)>>\<in\><around*|(|0,+\<infty\>|)>> to be
  determined and <math|<wide|f|~><rsup|<around*|(|n|)>>> has <math|n>
  parameters. Since there is only one model in a model family for each
  <math|n>, <math|f<rsup|<around*|(|n|)>>> and
  <math|<wide|f|~><rsup|<around*|(|n|)>>> share the same functional form, and
  <math|<wide|f|~><rsup|<around*|(|n|)>><around*|(|x;0|)>=0> for all
  <math|x>. In this way, we construct a new model with <math|2n> parameters.
  The loss function now becomes

  <\equation*>
    L<rsup|<around*|(|2n|)>><around*|(|\<theta\>,<wide|\<theta\>|~>|)>\<assign\><big|int>\<mathd\>x\<mathd\>y
    \ p<around*|(|x,y|)><around*|(|f<rsup|<around*|(|n|)>><around*|(|x;\<theta\>|)>+r<rsup|<around*|(|n|)>>
    <wide|f|~><rsup|<around*|(|n|)>><around*|(|x;<wide|\<theta\>|~>|)>-y|)><rsup|2>.
  </equation*>

  By inserting a unity, the RHS becomes

  <\equation*>
    <big|int>\<mathd\>x\<mathd\>y \ p<around*|(|x,y|)><around*|[|r<rsup|<around*|(|n|)>><big|int>\<mathd\><wide|y|~>
    \<delta\><around*|(|r <wide|y|~>-y+f<rsup|<around*|(|n|)>><around*|(|x;\<theta\>|)>|)>|]><around*|(|f<rsup|<around*|(|n|)>><around*|(|x;\<theta\>|)>+r<rsup|<around*|(|n|)>>
    <wide|f|~><rsup|<around*|(|n|)>><around*|(|x;<wide|\<theta\>|~>|)>-y|)><rsup|2>,
  </equation*>

  where the <math|<around*|[|\<ldots\>|]>=1>. Exchanging the integrals of
  <math|y> and of <math|<wide|y|~>>, we get

  <\equation*>
    <big|int>\<mathd\>x\<mathd\><wide|y|~><around*|[|r<rsup|<around*|(|n|)>><big|int>\<mathd\>y
    \ p<around*|(|x,y|)> \<delta\><around*|(|r<rsup|<around*|(|n|)>>
    <wide|y|~>-y+f<rsup|<around*|(|n|)>><around*|(|x;\<theta\>|)>|)>|]><around*|(|r<rsup|<around*|(|n|)>>
    <wide|f|~><rsup|<around*|(|n|)>><around*|(|x;<wide|\<theta\>|~>|)>-r<rsup|<around*|(|n|)>>
    <wide|y|~>|)><rsup|2>.
  </equation*>

  The term in the bracket is in fact a distribution:<\footnote>
    We have to show that <math|<wide|p|~><rsup|<around*|(|n|)>><around*|(|\<ldots\>;\<theta\>|)>>
    is indeed a distribution, that is, for any <math|\<theta\>>,
    <math|<big|int>\<mathd\>x\<mathd\><wide|y|~>
    <wide|p|~><rsup|<around*|(|n|)>><around*|(|x,<wide|y|~>;\<theta\>|)>=1>.
    Indeed,

    <\align>
      <tformat|<table|<row|<cell|>|<cell|<big|int>\<mathd\>x\<mathd\><wide|y|~>
      <wide|p|~><rsup|<around*|(|n|)>><around*|(|x,<wide|y|~>;\<theta\>|)>>>|<row|<cell|<around*|{|<wide|p|~><rsup|<around*|(|n|)>>\<assign\>\<cdots\>|}>=>|<cell|<big|int>\<mathd\>x\<mathd\><wide|y|~>
      <around*|[|r<rsup|<around*|(|n|)>><big|int>\<mathd\>y
      \ p<around*|(|x,y|)> \<delta\><around*|(|r<rsup|<around*|(|n|)>>
      <wide|y|~>-y+f<around*|(|x;\<theta\>|)>|)>|]>>>|<row|<cell|=>|<cell|<big|int>\<mathd\>x
      \<mathd\>y p<around*|(|x,y|)> <around*|[|r<rsup|<around*|(|n|)>><big|int>\<mathd\><wide|y|~>
      \<delta\><around*|(|r<rsup|<around*|(|n|)>>
      <wide|y|~>-y+f<around*|(|x;\<theta\>|)>|)>|]>>>|<row|<cell|<around*|{|<text|integrate
      over <math|<wide|y|~>>>|}>=>|<cell|<big|int>\<mathd\>x \<mathd\>y
      p<around*|(|x,y|)>>>|<row|<cell|<around*|{|<text|<math|p<rsub|D>><math|
      is a distribution>>|}>=>|<cell|1.>>>>
    </align>
  </footnote>

  <\equation*>
    <wide|p|~><rsup|<around*|(|n|)>><around*|(|x,<wide|y|~>;\<theta\>|)>\<assign\>r<rsup|<around*|(|n|)>><big|int>\<mathd\>y
    \ p<around*|(|x,y|)> \<delta\><around*|(|r<rsup|<around*|(|n|)>>
    <wide|y|~>-y+f<rsup|<around*|(|n|)>><around*|(|x;\<theta\>|)>|)>.
  </equation*>

  Plugging into the RHS, we find

  <\equation*>
    L<rsup|<around*|(|2n|)>><around*|(|\<theta\>,<wide|\<theta\>|~>|)>=r<rsup|<around*|(|n|)>><rsup|2>
    <big|int>\<mathd\>x \<mathd\><wide|y|~>
    <wide|p|~><rsup|<around*|(|n|)>><around*|(|x,<wide|y|~>;\<theta\>|)>
    <around*|(|<wide|f|~><rsup|<around*|(|n|)>><around*|(|x;<wide|\<theta\>|~>|)>-<wide|y|~>|)><rsup|2>.
  </equation*>

  The <math|<wide|y|~>=<around*|(|y-f<rsup|<around*|(|n|)>><around*|(|x;\<theta\>|)>|)>/r<rsup|<around*|(|n|)>>>
  is the rescaled residual error, and <math|<wide|P|~><rsup|<around*|(|n|)>>>
  describes its distribution.

  \;

  If we freeze the <math|\<theta\>> to be <math|\<theta\><rsub|\<star\>>> and
  only adjust the <math|<wide|\<theta\>|~>> while optimizing the model, then
  <math|<wide|p|~><rsup|<around*|(|n|)>>> depends only on <math|x> and
  <math|<wide|y|~>>, and <math|L<rsup|<around*|(|2n|)>>> depends only on
  <math|<wide|\<theta\>|~>>. Since <math|L<rsup|<around*|(|2n|)>><around*|(|\<theta\><rsub|\<star\>>,0|)>=L<rsub|\<star\>><rsup|<around*|(|n|)>>>,
  for formally going back to the original, let
  <math|r<rsup|<around*|(|n|)>>=<sqrt|L<rsub|\<star\>><rsup|<around*|(|n|)>>/L<rsub|0>>>,
  and we finally arrive at

  <\equation*>
    <wide|L|~><rsup|<around*|(|n|)>><around*|(|<wide|\<theta\>|~>|)>\<assign\><frac|L<rsub|0>|L<rsub|\<star\>><rsup|<around*|(|n|)>>>L<rsup|<around*|(|2n|)>><around*|(|\<theta\><rsub|\<star\>>,<wide|\<theta\>|~>|)>=<big|int>\<mathd\>x
    \<mathd\><wide|y|~> <wide|p|~><rsup|<around*|(|n|)>><around*|(|x,<wide|y|~>;\<theta\><rsub|\<star\>>|)>
    <around*|(|<wide|f|~><rsup|<around*|(|n|)>><around*|(|x;<wide|\<theta\>|~>|)>-<wide|y|~>|)><rsup|2>.
  </equation*>

  Comparing with the expression of <math|L<rsup|<around*|(|n|)>>>,
  <math|<wide|L|~><rsup|<around*|(|n|)>>> is formally the same as <math|L>,
  even with <math|<wide|L|~><rsup|<around*|(|n|)>><around*|(|0|)>=L<rsup|><around*|(|0|)>=L<rsub|0>>.
  The <math|<wide|f|~><rsup|<around*|(|n|)>>> shares the same funcitonal form
  with <math|f<rsup|<around*|(|n|)>>> since they are in the same model
  family. Thus, the only difference is that <math|p> is replaced by
  <math|<wide|p|~><rsup|<around*|(|n|)>><around*|(|\<ldots\>;\<theta\><rsub|\<star\>>|)>>.

  The minimum of <math|<wide|L|~><rsup|<around*|(|n|)>>> depends on the
  complexity of <math|<wide|P|~><rsup|<around*|(|n|)>><around*|(|\<theta\><rsub|\<star\>>|)>>,
  which characterizes the distribution of residual errors of
  <math|f<rsup|<around*|(|n|)>><around*|(|\<ldots\>;\<theta\><rsub|\<star\>>|)>>.
  Denoting <math|<wide|\<theta\>|~><rsub|\<star\>>\<assign\>argmin
  <wide|L|~><rsup|<around*|(|n|)>>> and <math|<wide|L|~><rsub|\<star\>><rsup|<around*|(|n|)>>\<assign\><wide|L|~><rsup|<around*|(|n|)>><around*|(|<wide|\<theta\>|~><rsub|\<star\>>|)>>,
  we have

  <\equation*>
    L<rsup|<around*|(|2n|)>><rsub|\<star\>>\<assign\>L<rsup|<around*|(|2n|)>><around*|(|\<theta\><rsub|\<star\>>,<wide|\<theta\>|~><rsub|\<star\>>|)>=<frac|L<rsup|<around*|(|n|)>><rsub|\<star\>>|L<rsub|0>><wide|L|~><rsup|<around*|(|n|)>><around*|(|<wide|\<theta\>|~><rsub|\<star\>>|)>=<frac|L<rsup|<around*|(|n|)>><rsub|\<star\>>|L<rsub|0>>
    <wide|L|~><rsub|\<star\>><rsup|<around*|(|n|)>>=L<rsup|<around*|(|n|)>><rsub|\<star\>>
    <frac|<wide|L|~><rsub|\<star\>><rsup|<around*|(|n|)>>|L<rsub|0>>.
  </equation*>

  In the last second equality, we insert the definition of
  <math|<wide|L|~><rsub|\<star\>><rsup|<around*|(|n|)>>>; and lastly, the
  expression is re-arranged.

  Even though <math|L<rsup|<around*|(|2n|)>><rsub|\<star\>>> or
  <math|L<rsup|<around*|(|2n|)>><around*|(|\<theta\><rsub|\<star\>>,<wide|\<theta\>|~><rsub|\<star\>>|)>>
  may not be the <math|min L<rsup|<around*|(|2n|)>><around*|(|\<theta\>,<wide|\<theta\>|~>|)>>,
  we suppose that the difference is negligible. That is, we suppose that the
  optimization by first on <math|\<theta\>> and then on
  <math|<wide|\<theta\>|~>> with <math|\<theta\>> frozen can approximate the
  best-fit, by which the <math|\<theta\>> and <math|<wide|\<theta\>|~>> are
  optimized synchronously. The reason is that the optimization of
  <math|\<theta\>> has taken most of the work that minimizes the loss, while
  optimizing <math|<wide|\<theta\>|~>> in the next step is just fine-tuning.
  Thus, we may expect that, when optimize <math|\<theta\>> and
  <math|<wide|\<theta\>|~>> synchronously, the change of <math|\<theta\>>
  from <math|\<theta\><rsub|\<star\>>> will be negligible.

  <subsection|Recursive Generalization>

  Let <math|P> a distribution of dataset and <math|F> a model family. Denote
  <math|L<rsub|0>\<assign\><big|int>\<mathd\>x\<mathd\>y p<around*|(|x,y|)>
  y<rsup|2>>. Initially, given some <math|n\<in\><around*|{|1,2,\<ldots\>|}>>,
  we use <math|f<rsup|<around*|(|n|)>><around*|(|x;\<theta\>|)>> in <math|F>
  to \Pfit\Q the dataset <math|P>. The loss is

  <\equation>
    L<rsup|<around*|(|n|)>><around*|(|\<theta\>|)>\<assign\><big|int>\<mathd\>x
    \<mathd\>y p<around*|(|x,y|)> <around*|(|f<rsup|<around*|(|n|)>><around*|(|x;\<theta\>|)>-y|)><rsup|2>.
  </equation>

  It has best-fit <math|\<theta\><rsub|\<star\>>\<assign\>argmin
  L<rsup|<around*|(|n|)>>> and <math|L<rsup|<around*|(|n|)>><rsub|\<star\>>\<assign\>L<rsup|<around*|(|n|)>><around*|(|\<theta\><rsub|\<star\>>|)>>.

  Now suppose that we have obtained the best-fit model with <math|k>
  parameters, <math|f<rsup|<around*|(|k|)>><around*|(|x;\<theta\><rsub|\<star\>>|)>>,
  with minimal loss <math|L<rsup|<around*|(|k|)>><rsub|\<star\>>>. To obtain
  that with <math|2k> parameters, we use <math|<wide|f|~><rsup|<around*|(|k|)>><around*|(|x;<wide|\<theta\>|~>|)>>
  in <math|F> to fit the residual error left by
  <math|f<rsup|<around*|(|k|)>><around*|(|x;\<theta\><rsub|\<star\>>|)>>. The
  distribution of rescaled residual error, <math|<wide|y|~>>, is given by

  <\equation>
    <wide|p|~><rsup|<around*|(|k|)>><around*|(|x,<wide|y|~>|)>\<assign\><sqrt|<frac|L<rsup|<around*|(|k|)>><rsub|\<star\>>|L<rsub|0>>><big|int>\<mathd\>y
    \ p<around*|(|x,y|)> \<delta\><around*|(|<sqrt|<frac|L<rsup|<around*|(|k|)>><rsub|\<star\>>|L<rsub|0>>>
    <wide|y|~>-y+f<rsup|<around*|(|k|)>><around*|(|x;\<theta\><rsub|\<star\>>|)>|)>.
  </equation>

  Then, the loss function for optimizing <math|<wide|f|~><rsup|<around*|(|k|)>><around*|(|x;<wide|\<theta\>|~>|)>>
  is derived as

  <\equation>
    <wide|L|~><rsup|<around*|(|k|)>><around*|(|<wide|\<theta\>|~>|)>\<assign\><frac|L<rsub|0>|L<rsub|\<star\>><rsup|<around*|(|k|)>>>L<rsup|<around*|(|2k|)>><around*|(|\<theta\><rsub|\<star\>>,<wide|\<theta\>|~>|)>=<big|int>\<mathd\>x
    \<mathd\>y <wide|p|~><rsup|<around*|(|k|)>><around*|(|x,y|)>
    <around*|(|<wide|f|~><rsup|<around*|(|k|)>><around*|(|x;<wide|\<theta\>|~>|)>-y|)><rsup|2>,
  </equation>

  which has <math|<wide|L|~><rsup|<around*|(|k|)>><around*|(|0|)>=L<rsub|0>>.
  Let <math|<wide|\<theta\>|~><rsub|\<star\>>\<assign\>argmin
  <wide|L|~><rsup|<around*|(|k|)>>> and <math|<wide|L|~><rsup|<around*|(|k|)>><rsub|\<star\>>\<assign\><wide|L|~><rsup|<around*|(|k|)>><around*|(|<wide|\<theta\>|~><rsub|\<star\>>|)>>.
  We suppose that (argued in the last paragraph in previous subsection)

  <\equation*>
    <around*|(|\<theta\><rsub|\<star\>>,<wide|\<theta\>|~><rsub|\<star\>>|)>\<approx\>argmin
    L<rsup|<around*|(|2k|)>>.
  </equation*>

  Thus, the best-fit model with <math|2k> parameters can be approximated by

  <\equation>
    f<rsup|<around*|(|k|)>><around*|(|x;\<theta\><rsub|\<star\>>|)>+<sqrt|<frac|L<rsup|<around*|(|k|)>><rsub|\<star\>>|L<rsub|0>>>
    <wide|f|~><rsup|<around*|(|k|)>><around*|(|x;<wide|\<theta\>|~><rsub|\<star\>>|)>;
  </equation>

  and the minimum of loss function for this model becomes

  <\equation*>
    L<rsup|<around*|(|2k|)>><rsub|\<star\>>=L<rsub|\<star\>><rsup|<around*|(|k|)>>
    <frac|<wide|L|~><rsup|<around*|(|k|)>><rsub|\<star\>>|L<rsub|0>>.
  </equation*>

  So, we have, for any <math|m,n\<in\><around*|{|1,2,\<ldots\>|}>>,

  <\equation>
    L<rsup|<around*|(|2<rsup|m> n|)>><rsub|\<star\>>=L<rsup|<around*|(|n|)>><rsub|\<star\>>
    <frac|<wide|L|~><rsup|<around*|(|n|)>><rsub|\<star\>>|L<rsub|0>><frac|<wide|L|~><rsup|<around*|(|2n|)>><rsub|\<star\>>|L<rsub|0>>\<cdots\><frac|<wide|L|~><rsup|<around*|(|2<rsup|<around*|(|m-1|)>>
    n|)>><rsub|\<star\>>|L<rsub|0>>.<label|equation:L star regression>
  </equation>

  <subsection|Digression: Complete Model Family>

  You can construct a free vector space based on the model family, thus
  <math|f<rsup|<around*|(|k|)>>> is an element of this vector space. If, for
  each model with <math|k> parameters, there exists a linear combinition of
  elements in the model family that equals to the model, then we say the
  model family forms a complete basis for the collection of models. How can a
  model family be complete?

  <section|Power-Law>

  <subsection|Power-Law Arises From Self-Similarity of Complexity (TODO)>

  We have derived the relation between <math|L<rsup|<around*|(|2<rsup|m>
  n|)>><rsub|\<star\>>/L<rsub|0>> and <math|2<rsup|m> n>. We wonder when the
  power-law relation between <math|L<rsup|<around*|(|2<rsup|m>
  n|)>><rsub|\<star\>>/L<rsub|0>> and <math|2<rsup|m> n> may arise.

  It is well known that a function <math|f<around*|(|x|)>:\<bbb-R\>\<rightarrow\>\<bbb-R\>>
  is power-law, which means there is a <math|\<gamma\>\<in\>\<bbb-R\>> such
  that <math|f<around*|(|x|)>=x<rsup|\<gamma\>>>, if and only if there is a
  function <math|k:\<bbb-R\><rsub|+>\<rightarrow\>\<bbb-R\><rsub|+>> such
  that, for <math|\<forall\>s,x>, <math|f<around*|(|s x|)>=k<around*|(|s|)>
  f<around*|(|x|)>>. Applying to our situation, we find that, given <math|n>,
  the condition for power-law to arise is that
  <math|L<rsup|<around*|(|2<rsup|<around*|(|m+1|)>>
  n|)>><rsub|\<star\>>/L<rsup|<around*|(|2<rsup|m> n|)>><rsub|\<star\>>> is
  independent of <math|m>. From equation (<reference|equation:L star
  regression>), we have

  <\equation*>
    <frac|L<rsup|<around*|(|2<rsup|<around*|(|m+1|)>>
    n|)>><rsub|\<star\>>|L<rsup|<around*|(|2<rsup|m>
    n|)>><rsub|\<star\>>>=<frac|<wide|L|~><rsup|<around*|(|2<rsup|m>
    n|)>><rsub|\<star\>>|L<rsub|0>>,
  </equation*>

  indicating that <math|><math|<wide|L|~><rsup|<around*|(|2<rsup|m>
  n|)>><rsub|\<star\>>> shall be independent of <math|m>. Recall that
  <math|<wide|L|~><rsup|<around*|(|2<rsup|m>
  n|)>><around*|(|<wide|\<theta\>|~>|)>>, as the loss of residual error of
  <math|f<rsup|<around*|(|2<rsup|m> n|)>><around*|(|x;\<theta\><rsub|\<star\>>|)>>,
  has a \Pstandard\Q format. Thus, its minimum,
  <math|<wide|L|~><rsup|<around*|(|2<rsup|m> n|)>><rsub|\<star\>>>, is
  completely determined by the complexity of
  <math|<wide|P|~><rsup|<around*|(|2<rsup|m>
  n|)>><around*|(|\<theta\><rsub|\<star\>>|)>>. So, that
  <math|<wide|L|~><rsup|<around*|(|2<rsup|m> n|)>><rsub|\<star\>>> is
  independent of <math|m> implies that the complexity of
  <math|<wide|P|~><rsup|<around*|(|2k|)>><around*|(|\<theta\><rsub|\<star\>>|)>>
  is more complex than <math|<wide|P|~><rsup|<around*|(|k|)>><around*|(|\<theta\><rsub|\<star\>>|)>>
  because, for approaching the same minimum of loss function, we need double
  number of parameters for <math|<wide|P|~><rsup|<around*|(|2k|)>><around*|(|\<theta\><rsub|\<star\>>|)>>
  comparing with <math|<wide|P|~><rsup|<around*|(|k|)>><around*|(|\<theta\><rsub|\<star\>>|)>>.
  Shortly, the complexity of residual error examined by the model family
  always doubles TODO
</body>

<\initial>
  <\collection>
    <associate|page-medium|paper>
  </collection>
</initial>

<\references>
  <\collection>
    <associate|auto-1|<tuple|1|1>>
    <associate|auto-10|<tuple|1.3|4>>
    <associate|auto-11|<tuple|1.3.1|4>>
    <associate|auto-12|<tuple|1.3.1|?>>
    <associate|auto-2|<tuple|1.1|1>>
    <associate|auto-3|<tuple|1.1.1|1>>
    <associate|auto-4|<tuple|1.1.2|1>>
    <associate|auto-5|<tuple|1.1.3|1>>
    <associate|auto-6|<tuple|1.2|2>>
    <associate|auto-7|<tuple|1.2.1|2>>
    <associate|auto-8|<tuple|1.2.2|2>>
    <associate|auto-9|<tuple|1.2.3|3>>
    <associate|equation:L star regression|<tuple|1.8|4>>
    <associate|footnote-1.1|<tuple|1.1|2>>
    <associate|footnote-1.2|<tuple|1.2|?>>
    <associate|footnr-1.1|<tuple|1.1|2>>
    <associate|footnr-1.2|<tuple|1.2|?>>
  </collection>
</references>

<\auxiliary>
  <\collection>
    <\associate|toc>
      <vspace*|1fn><with|font-series|<quote|bold>|math-font-series|<quote|bold>|1<space|2spc>Scaling
      and Power-Law> <datoms|<macro|x|<repeat|<arg|x>|<with|font-series|medium|<with|font-size|1|<space|0.2fn>.<space|0.2fn>>>>>|<htab|5mm>>
      <no-break><pageref|auto-1><vspace|0.5fn>

      1.1<space|2spc>Complexity <datoms|<macro|x|<repeat|<arg|x>|<with|font-series|medium|<with|font-size|1|<space|0.2fn>.<space|0.2fn>>>>>|<htab|5mm>>
      <no-break><pageref|auto-2>

      <with|par-left|<quote|1tab>|1.1.1<space|2spc>Distribution Is a Data
      Generator <datoms|<macro|x|<repeat|<arg|x>|<with|font-series|medium|<with|font-size|1|<space|0.2fn>.<space|0.2fn>>>>>|<htab|5mm>>
      <no-break><pageref|auto-3>>

      <with|par-left|<quote|1tab>|1.1.2<space|2spc>Model Is a Functional Form
      <datoms|<macro|x|<repeat|<arg|x>|<with|font-series|medium|<with|font-size|1|<space|0.2fn>.<space|0.2fn>>>>>|<htab|5mm>>
      <no-break><pageref|auto-4>>

      <with|par-left|<quote|1tab>|1.1.3<space|2spc>Complexity of Dataset
      Examed by Model <datoms|<macro|x|<repeat|<arg|x>|<with|font-series|medium|<with|font-size|1|<space|0.2fn>.<space|0.2fn>>>>>|<htab|5mm>>
      <no-break><pageref|auto-5>>

      1.2<space|2spc>Relation between Minimum of Loss and Number of
      Parameters: A Simple Example <datoms|<macro|x|<repeat|<arg|x>|<with|font-series|medium|<with|font-size|1|<space|0.2fn>.<space|0.2fn>>>>>|<htab|5mm>>
      <no-break><pageref|auto-6>

      <with|par-left|<quote|1tab>|1.2.1<space|2spc>Notations
      <datoms|<macro|x|<repeat|<arg|x>|<with|font-series|medium|<with|font-size|1|<space|0.2fn>.<space|0.2fn>>>>>|<htab|5mm>>
      <no-break><pageref|auto-7>>

      <with|par-left|<quote|1tab>|1.2.2<space|2spc>An Extending-Rescaling
      Process of Loss Function <datoms|<macro|x|<repeat|<arg|x>|<with|font-series|medium|<with|font-size|1|<space|0.2fn>.<space|0.2fn>>>>>|<htab|5mm>>
      <no-break><pageref|auto-8>>

      <with|par-left|<quote|1tab>|1.2.3<space|2spc>Generalizing to More
      Parameters <datoms|<macro|x|<repeat|<arg|x>|<with|font-series|medium|<with|font-size|1|<space|0.2fn>.<space|0.2fn>>>>>|<htab|5mm>>
      <no-break><pageref|auto-9>>

      1.3<space|2spc>Power-Law <datoms|<macro|x|<repeat|<arg|x>|<with|font-series|medium|<with|font-size|1|<space|0.2fn>.<space|0.2fn>>>>>|<htab|5mm>>
      <no-break><pageref|auto-10>

      <with|par-left|<quote|1tab>|1.3.1<space|2spc>Power-Law Arises From
      Self-Similarity of Complexity <datoms|<macro|x|<repeat|<arg|x>|<with|font-series|medium|<with|font-size|1|<space|0.2fn>.<space|0.2fn>>>>>|<htab|5mm>>
      <no-break><pageref|auto-11>>
    </associate>
  </collection>
</auxiliary>