<TeXmacs|2.1>

<style|book>

<\body>
  <chapter|Scaling and Power-Law>

  <section|Complexity>

  In this section, we introduce the complexity of a dataset examined by a
  model. It is a preparation for the discussion of power-law.

  <subsection|Distribution Is a Data Generator>

  Ideally, a dataset is described by a distribution. This distribution is in
  fact a data generator. For example, Internet is a generator of texture
  data. Expression <math|x\<sim\>P<rsub|<text|Internet>>> means the text
  <math|x> is generated by Internet. We cannot say how many texture data are
  there on the Internet, since Internet can generate a new one at any time.
  Thus, <math|P<rsub|<text|Internet>>> is really a data generator; and we can
  generate infinite data from <math|P<rsub|<text|Internet>>>, theoretically.

  <subsection|Model Is a Functional Form>

  Usually, we deal with models with finite parameters. But, ideally, we can
  think of a model defined with infinity many parameters, with all but finite
  number of them vanishing. For example, we define a model
  <math|f<around*|(|x;\<theta\>|)>> to be polynomial with <math|\<theta\>>
  the collection of coefficients. Thus, <math|f<around*|(|x;\<theta\>|)>\<assign\><big|sum><rsub|k=1><rsup|+\<infty\>>\<theta\><rsub|k>
  x<rsup|k>> where only finite number of <math|\<theta\><rsub|k>> are not
  zero. What we have defined is the functional form of the model <math|f>.

  <subsection|Complexity of Dataset Examined by Model>

  Let <math|P> a distribution of dataset and <math|f> a functional form of
  model. The loss function of a supervised learning task is

  <\equation*>
    L<around*|(|\<theta\>|)>=<big|int>\<mathd\>x\<mathd\>y
    \ p<around*|(|x,y|)>d<around*|(|y,f<around*|(|x;\<theta\>|)>|)>,
  </equation*>

  where <math|d> is some distance between target and model prediction (for
  regression task, it is the mean squared error; and for classification task,
  it is the relative entropy).

  Without losing generality, suppose that <math|f<around*|(|x;0|)>=0> for
  <math|\<forall\>x\<in\>\<bbb-R\>>. Otherwise, we can re-define
  <math|f<around*|(|x;\<theta\>|)>> by <math|f<around*|(|x;\<theta\>|)>-f<around*|(|x;0|)>>.
  Thus, <math|L<rsub|0>\<assign\>L<around*|(|0|)>=<big|int>\<mathd\>x\<mathd\>y
  \ p<around*|(|x,y|)>d<around*|(|y,0|)>>. Let <math|n> the number of
  parameters of <math|f>, thus <math|\<theta\>\<in\>\<bbb-R\><rsup|n>>. By
  optimization, we have <math|\<theta\><rsub|\<star\>>\<assign\>argmin<rsub|\<theta\>\<in\>\<bbb-R\><rsup|n>>L<around*|(|\<theta\>|)>>
  and <math|L<rsub|\<star\>>\<assign\>L<around*|(|\<theta\><rsub|\<star\>>|)>>.
  The <math|L<rsub|\<star\>>> has the dimension <math|<around*|[|L|]>>. The
  dimensionless quantity we are to consider shall be
  <math|L<rsub|\<star\>>/L<rsub|0>>.

  Given the model <math|f>, the relation between the number of parameters
  <math|n> and the (relative) minimal loss <math|L<rsub|\<star\>>/L<rsub|0>>
  reflects the complexity of dataset <math|P>. With a fixed
  <math|L<rsub|\<star\>>/L<rsub|0>> as the target of optimization, the more
  model parameters we use for obtaining the
  <math|L<rsub|\<star\>>/L<rsub|0>>, the more complicated the <math|P> should
  be. Or with a fixed <math|n> as the model size, a simpler dataset <math|P>
  will furnish a smaller best-fit loss <math|L<rsub|\<star\>>/L<rsub|0>>. So,
  we call this relation between <math|n> and
  <math|L<rsub|\<star\>>/L<rsub|0>> the <with|font-series|bold|complexity> of
  dataset examined by the model <math|f>. The model <math|f> can be viewed as
  a ruler that exams the complexity of different dataset.

  To be explicit and thorough, we add a formal definition in the end of our
  discussion.

  <\definition>
    [Complexity of Dataset Examined by Model] Let <math|f> a functional form
    of model and <math|P,Q> are two distributions of dataset. Let
    <math|L<rsub|P><around*|(|\<theta\>|)>\<assign\><big|int>\<mathd\>x\<mathd\>y
    \ p<around*|(|x,y|)>d<around*|(|y,f<around*|(|x;\<theta\>|)>|)>> and the
    same for <math|L<rsub|Q>>. Complexity is a partial order relation:
    <math|P> is said to be more complicated than <math|Q> if, for
    <math|\<forall\>\<epsilon\>\<gtr\>0>, the minimal <math|n> such that
    <math|min<rsub|\<theta\>\<in\>\<bbb-R\><rsup|n>>L<rsub|P><around*|(|\<theta\>|)>\<leqslant\>\<epsilon\>
    L<rsub|P><around*|(|0|)>> is greater than the minimal <math|m> such that
    <math|min<rsub|\<theta\>\<in\>\<bbb-R\><rsup|m>>L<rsub|Q><around*|(|\<theta\>|)>\<leqslant\>\<epsilon\>
    L<rsub|Q><around*|(|0|)>>.
  </definition>

  <section|Relation between Minimum of Loss and Number of Parameters: A
  Simple Example>

  We are to exam how the <math|L<rsub|\<star\>>/L<rsub|0>> changes with
  <math|n> increasing. To do so, we have to employ proper notations.

  <subsection|Notations>

  A model with <math|n> parameters is denoted by
  <math|f<rsup|<around*|(|n|)>><around*|(|x;\<theta\><rsup|<around*|(|n|)>>|)>>.
  Usually, we omit the <math|<around*|(|n|)>> in
  <math|\<theta\><rsup|<around*|(|n|)>>>, thus
  <math|f<rsup|<around*|(|n|)>><around*|(|x;\<theta\>|)>>, since it is
  obvious that <math|\<theta\>\<in\>\<bbb-R\><rsup|n>>. Its loss is denoted
  by <math|L<rsup|<around*|(|n|)>><around*|(|\<theta\><rsup|<around*|(|n|)>>|)>>
  or <math|L<rsup|<around*|(|n|)>><around*|(|\<theta\>|)>>, thus
  <math|L<rsup|<around*|(|n|)>><rsub|\<star\>>> and
  <math|\<theta\><rsup|<around*|(|n|)>><rsub|\<star\>>> for its <math|min>
  and <math|argmin> respectively.

  <subsection|An Extending-Rescaling Process of Loss Function>

  For simplicity, let us consider an one-dimensional regression task. Given
  by a dataset distribution <math|P> and the functional form of model
  <math|f>, the loss function is

  <\equation*>
    L<around*|(|\<theta\>|)>=<big|int>\<mathd\>x\<mathd\>y
    \ p<around*|(|x,y|)><around*|(|f<around*|(|x;\<theta\>|)>-y|)><rsup|2>.
  </equation*>

  We wonder how the <math|L<rsup|<around*|(|n|)>><rsub|\<star\>>/L<rsub|0>>
  changes with <math|n\<rightarrow\>2n>. The point is first train a baseline
  model with <math|f<rsup|<around*|(|n|)>><around*|(|x;\<theta\>|)>>, which
  furnishes a residual error. Then, enlarge the
  <math|f<rsup|<around*|(|n|)>>> to a model with <math|2n> parameters by
  combining with another <math|<wide|f|~><rsup|<around*|(|n|)>>> which fits
  the residual error. We then estimate the minimum of loss for the combined
  model.

  Explicitly, we enlarge the space of parameters by introducing a new model
  <math|<wide|f|~><rsup|<around*|(|n|)>>> and replacing
  <math|f<rsup|<around*|(|n|)>><around*|(|x;\<theta\>|)>\<rightarrow\>f<rsup|<around*|(|n|)>><around*|(|x;\<theta\>|)>+r<rsup|<around*|(|n|)>>
  <wide|f|~><rsup|<around*|(|n|)>><around*|(|x;<wide|\<theta\>|~>|)>>, where
  <math|r\<in\><around*|(|0,+\<infty\>|)>> to be determined and
  <math|<wide|f|~><rsup|<around*|(|n|)>>> has <math|n> parameters. Again, we
  suppose that <math|<wide|f|~><rsup|<around*|(|n|)>><around*|(|*x;0|)>=0>
  for all <math|x>. In this way, we construct a new model with <math|2n>
  parameters. The loss function now becomes

  <\equation*>
    L<rsup|<around*|(|2n|)>><around*|(|\<theta\>,<wide|\<theta\>|~>|)>\<assign\><big|int>\<mathd\>x\<mathd\>y
    \ p<around*|(|x,y|)><around*|(|f<rsup|<around*|(|n|)>><around*|(|x;\<theta\>|)>+r<rsup|<around*|(|n|)>>
    <wide|f|~><rsup|<around*|(|n|)>><around*|(|x;<wide|\<theta\>|~>|)>-y|)><rsup|2>.
  </equation*>

  By inserting a unity, the RHS becomes

  <\equation*>
    <big|int>\<mathd\>x\<mathd\>y \ p<around*|(|x,y|)><around*|[|r<rsup|<around*|(|n|)>><big|int>\<mathd\><wide|y|~>
    \<delta\><around*|(|r <wide|y|~>-y+f<rsup|<around*|(|n|)>><around*|(|x;\<theta\>|)>|)>|]><around*|(|f<rsup|<around*|(|n|)>><around*|(|x;\<theta\>|)>+r<rsup|<around*|(|n|)>>
    <wide|f|~><rsup|<around*|(|n|)>><around*|(|x;<wide|\<theta\>|~>|)>-y|)><rsup|2>,
  </equation*>

  where the <math|<around*|[|\<ldots\>|]>=1>. Exchanging the integrals of
  <math|y> and of <math|<wide|y|~>>, we get

  <\equation*>
    <big|int>\<mathd\>x\<mathd\><wide|y|~><around*|[|r<rsup|<around*|(|n|)>><big|int>\<mathd\>y
    \ p<around*|(|x,y|)> \<delta\><around*|(|r<rsup|<around*|(|n|)>>
    <wide|y|~>-y+f<rsup|<around*|(|n|)>><around*|(|x;\<theta\>|)>|)>|]><around*|(|r<rsup|<around*|(|n|)>>
    <wide|f|~><rsup|<around*|(|n|)>><around*|(|x;<wide|\<theta\>|~>|)>-r<rsup|<around*|(|n|)>>
    <wide|y|~>|)><rsup|2>.
  </equation*>

  The term in the bracket is in fact a distribution:<\footnote>
    We have to show that <math|<wide|p|~><rsup|<around*|(|n|)>><around*|(|\<ldots\>;\<theta\>|)>>
    is indeed a distribution, that is, for any <math|\<theta\>>,
    <math|<big|int>\<mathd\>x\<mathd\><wide|y|~>
    <wide|p|~><rsup|<around*|(|n|)>><around*|(|x,<wide|y|~>;\<theta\>|)>=1>.
    Indeed,

    <\align>
      <tformat|<table|<row|<cell|>|<cell|<big|int>\<mathd\>x\<mathd\><wide|y|~>
      <wide|p|~><rsup|<around*|(|n|)>><around*|(|x,<wide|y|~>;\<theta\>|)>>>|<row|<cell|<around*|{|<wide|p|~><rsup|<around*|(|n|)>>\<assign\>\<cdots\>|}>=>|<cell|<big|int>\<mathd\>x\<mathd\><wide|y|~>
      <around*|[|r<rsup|<around*|(|n|)>><big|int>\<mathd\>y
      \ p<around*|(|x,y|)> \<delta\><around*|(|r<rsup|<around*|(|n|)>>
      <wide|y|~>-y+f<around*|(|x;\<theta\>|)>|)>|]>>>|<row|<cell|=>|<cell|<big|int>\<mathd\>x
      \<mathd\>y p<around*|(|x,y|)> <around*|[|r<rsup|<around*|(|n|)>><big|int>\<mathd\><wide|y|~>
      \<delta\><around*|(|r<rsup|<around*|(|n|)>>
      <wide|y|~>-y+f<around*|(|x;\<theta\>|)>|)>|]>>>|<row|<cell|<around*|{|<text|integrate
      over <math|<wide|y|~>>>|}>=>|<cell|<big|int>\<mathd\>x \<mathd\>y
      p<around*|(|x,y|)>>>|<row|<cell|<around*|{|<text|<math|p<rsub|D>><math|
      is a distribution>>|}>=>|<cell|1.>>>>
    </align>
  </footnote>

  <\equation*>
    <wide|p|~><rsup|<around*|(|n|)>><around*|(|x,<wide|y|~>;\<theta\>|)>\<assign\>r<rsup|<around*|(|n|)>><big|int>\<mathd\>y
    \ p<around*|(|x,y|)> \<delta\><around*|(|r<rsup|<around*|(|n|)>>
    <wide|y|~>-y+f<rsup|<around*|(|n|)>><around*|(|x;\<theta\>|)>|)>.
  </equation*>

  Plugging into the RHS, we find

  <\equation*>
    L<rsup|<around*|(|2n|)>><around*|(|\<theta\>,<wide|\<theta\>|~>|)>=r<rsup|<around*|(|n|)>><rsup|2>
    <big|int>\<mathd\>x \<mathd\><wide|y|~>
    <wide|p|~><rsup|<around*|(|n|)>><around*|(|x,<wide|y|~>;\<theta\>|)>
    <around*|(|<wide|f|~><rsup|<around*|(|n|)>><around*|(|x;<wide|\<theta\>|~>|)>-<wide|y|~>|)><rsup|2>.
  </equation*>

  The <math|<wide|y|~>=<around*|(|y-f<rsup|<around*|(|n|)>><around*|(|x;\<theta\>|)>|)>/r<rsup|<around*|(|n|)>>>
  is the rescaled residual error, and <math|<wide|P|~><rsup|<around*|(|n|)>>>
  describes its distribution.

  \;

  If we freeze the <math|\<theta\>> to be <math|\<theta\><rsub|\<star\>>> and
  only adjust the <math|<wide|\<theta\>|~>> while optimizing the model, then
  <math|<wide|p|~><rsup|<around*|(|n|)>>> depends only on <math|x> and
  <math|<wide|y|~>>, and <math|L<rsup|<around*|(|2n|)>>> depends only on
  <math|<wide|\<theta\>|~>>. Since <math|L<rsup|<around*|(|2n|)>><around*|(|\<theta\><rsub|\<star\>>,0|)>=L<rsub|\<star\>><rsup|<around*|(|n|)>>>,
  for formally going back to the original, let
  <math|r<rsup|<around*|(|n|)>>=<sqrt|L<rsub|\<star\>><rsup|<around*|(|n|)>>/L<rsub|0>>>,
  and we finally arrive at

  <\equation*>
    <wide|L|~><rsup|<around*|(|n|)>><around*|(|<wide|\<theta\>|~>|)>\<assign\><frac|L<rsub|0>|L<rsub|\<star\>><rsup|<around*|(|n|)>>>L<rsup|<around*|(|2n|)>><around*|(|\<theta\><rsub|\<star\>>,<wide|\<theta\>|~>|)>=<big|int>\<mathd\>x
    \<mathd\><wide|y|~> <wide|p|~><rsup|<around*|(|n|)>><around*|(|x,<wide|y|~>;\<theta\><rsub|\<star\>>|)>
    <around*|(|<wide|f|~><rsup|<around*|(|n|)>><around*|(|x;<wide|\<theta\>|~>|)>-<wide|y|~>|)><rsup|2>.
  </equation*>

  Comparing with the expression of <math|L<rsup|<around*|(|n|)>>>,
  <math|<wide|L|~><rsup|<around*|(|n|)>>> is formally the same as <math|L>,
  even with <math|<wide|L|~><rsup|<around*|(|n|)>><around*|(|0|)>=L<rsup|><around*|(|0|)>=L<rsub|0>>.
  The <math|<wide|f|~>> is equivalent to <math|f> since their spaces of
  parameters are both <math|n>-dimensional. Thus, the only difference is that
  <math|p> is replaced by <math|<wide|p|~><rsup|<around*|(|n|)>><around*|(|\<ldots\>;\<theta\><rsub|\<star\>>|)>>.

  The minimum of <math|<wide|L|~><rsup|<around*|(|n|)>>> depends on the
  complexity of <math|<wide|P|~><rsup|<around*|(|n|)>><around*|(|\<theta\><rsub|\<star\>>|)>>,
  which characterizes the distribution of residual errors of
  <math|f<rsup|<around*|(|n|)>><around*|(|\<ldots\>;\<theta\><rsub|\<star\>>|)>>.
  Denoting <math|<wide|\<theta\>|~><rsub|\<star\>>\<assign\>argmin<rsub|<wide|\<theta\>|~>\<in\>\<bbb-R\><rsup|n>><wide|L|~><rsup|<around*|(|n|)>><around*|(|<wide|\<theta\>|~>|)>>
  and <math|<wide|L|~><rsub|\<star\>><rsup|<around*|(|n|)>>\<assign\><wide|L|~><rsup|<around*|(|n|)>><around*|(|<wide|\<theta\>|~><rsub|\<star\>>|)>>,
  we have

  <\equation*>
    L<rsup|<around*|(|2n|)>><rsub|\<star\>>\<assign\>L<rsup|<around*|(|2n|)>><around*|(|\<theta\><rsub|\<star\>>,<wide|\<theta\>|~><rsub|\<star\>>|)>=<frac|L<rsup|<around*|(|n|)>><rsub|\<star\>>|L<rsub|0>><wide|L|~><rsup|<around*|(|n|)>><around*|(|<wide|\<theta\>|~><rsub|\<star\>>|)>=<frac|L<rsup|<around*|(|n|)>><rsub|\<star\>>|L<rsub|0>>
    <wide|L|~><rsub|\<star\>><rsup|<around*|(|n|)>>=L<rsup|<around*|(|n|)>><rsub|\<star\>>
    <frac|<wide|L|~><rsub|\<star\>><rsup|<around*|(|n|)>>|L<rsub|0>>.
  </equation*>

  In the penult equality, we insert the definition of
  <math|<wide|L|~><rsub|\<star\>><rsup|<around*|(|n|)>>>; and lastly, the
  expression is re-arranged.

  Even though <math|L<rsup|<around*|(|2n|)>><rsub|\<star\>>> or
  <math|L<rsup|<around*|(|2n|)>><around*|(|\<theta\><rsub|\<star\>>,<wide|\<theta\>|~><rsub|\<star\>>|)>>
  may not be the <math|min<rsub|<around*|(|\<theta\>,<wide|\<theta\>|~>|)>\<in\>\<bbb-R\><rsup|2n>>L<rsup|<around*|(|2n|)>><around*|(|\<theta\>,<wide|\<theta\>|~>|)>>,
  we suppose that the difference is negligible. That is, we suppose that the
  optimization by first on <math|\<theta\>> and then on
  <math|<wide|\<theta\>|~>> with <math|\<theta\>> frozen can approximate the
  best-fit, by which the <math|\<theta\>> and <math|<wide|\<theta\>|~>> are
  optimized synchronously. The reason is that the optimization of
  <math|\<theta\>> has taken most of the work that minimizes the loss, while
  optimizing <math|<wide|\<theta\>|~>> in the next step is just fine-tuning.
  Thus, we may expect that, when optimize <math|\<theta\>> and
  <math|<wide|\<theta\>|~>> synchronously, the change of <math|\<theta\>>
  from <math|\<theta\><rsub|\<star\>>> will be negligible.

  <subsection|Generalizing to More Parameters>

  Now, we consider <math|4n>-dimensional space of parameters. To do this, we
  consider <math|f<rsup|<around*|(|n|)>><around*|(|x;\<theta\><rsup|<around*|(|n|)>>|)>+<wide|f|~><rsup|<around*|(|n|)>><around*|(|x;<wide|\<theta\>|~><rsup|<around*|(|n|)>>|)>>
  as the baseline model, that is, <math|f<rsup|<around*|(|2n|)>><around*|(|x;\<theta\>|)>\<assign\>f<rsup|<around*|(|n|)>><around*|(|x;\<theta\><rsup|<around*|(|n|)>>|)>+r<rsup|<around*|(|n|)>>
  <wide|f|~><rsup|<around*|(|n|)>><around*|(|x;<wide|\<theta\>|~><rsup|<around*|(|n|)>>|)>>
  and <math|\<theta\>\<assign\><around*|(|\<theta\><rsup|<around*|(|n|)>>,<wide|\<theta\>|~><rsup|<around*|(|n|)>>|)>\<in\>\<bbb-R\><rsup|2n>>.
  Thus, from what we have derived, <math|L<rsup|<around*|(|2n|)>><rsub|\<star\>>=L<rsup|<around*|(|n|)>><rsub|\<star\>>
  <around*|(|<wide|L|~><rsup|<around*|(|n|)>><rsub|\<star\>>/L<rsub|0>|)>>.

  Then, we repeat the previous steps, or simply replace <math|n> by
  <math|2n>. We sketch out the main steps here. As before, replace
  <math|f<rsup|<around*|(|2n|)>><around*|(|x;\<theta\>|)>> by
  <math|f<rsup|<around*|(|2n|)>><around*|(|x;\<theta\>|)>+r<rsup|<around*|(|2n|)>>
  <wide|f|~><rsup|<around*|(|2n|)>><around*|(|x;<wide|\<theta\>|~>|)>> where
  <math|r\<in\><around*|(|0,+\<infty\>|)>> to be determined. Thus the loss
  <math|L<rsup|<around*|(|4n|)>><around*|(|\<theta\>,<wide|\<theta\>|~>|)>\<assign\><big|int>\<mathd\>x\<mathd\>y
  \ p<around*|(|x,y|)><around*|(|f<rsup|<around*|(|2n|)>><around*|(|x;\<theta\>|)>+r<rsup|<around*|(|2n|)>>
  <wide|f|~><rsup|<around*|(|2n|)>><around*|(|x;<wide|\<theta\>|~>|)>-y|)><rsup|2>>.
  Following the same steps, we arrive at

  <\equation*>
    <wide|p|~><rsup|<around*|(|2n|)>><around*|(|x,<wide|y|~>;\<theta\>|)>\<assign\>r<rsup|<around*|(|2n|)>><big|int>\<mathd\>y
    \ p<around*|(|x,y|)> \<delta\><around*|(|r<rsup|<around*|(|2n|)>>
    <wide|y|~>-y+f<rsup|<around*|(|2n|)>><around*|(|x;\<theta\>|)>|)>
  </equation*>

  which characterizes the distribution of the (rescaled) residual error of
  <math|f<rsup|<around*|(|2n|)>><around*|(|x;\<theta\>|)>>, and

  <\equation*>
    L<rsup|<around*|(|4n|)>><around*|(|\<theta\>,<wide|\<theta\>|~>|)>=r<rsup|<around*|(|2n|)>><rsup|2>
    <big|int>\<mathd\>x \<mathd\><wide|y|~>
    <wide|p|~><rsup|<around*|(|2n|)>><around*|(|x,<wide|y|~>;\<theta\>|)>
    <around*|(|<wide|f|~><rsup|<around*|(|2n|)>><around*|(|x;<wide|\<theta\>|~>|)>-<wide|y|~>|)><rsup|2>.
  </equation*>

  Again, by denoting

  \;

  <\equation*>
    <wide|L|~><rsup|<around*|(|2n|)>><around*|(|<wide|\<theta\>|~>|)>\<assign\><frac|L<rsub|0>|L<rsup|<around*|(|2n|)>><rsub|\<star\>>>L<rsup|<around*|(|4n|)>><around*|(|\<theta\><rsub|\<star\>>,<wide|\<theta\>|~>|)>=<big|int>\<mathd\>x
    \<mathd\><wide|y|~> <wide|p|~><rsup|<around*|(|2n|)>><around*|(|x,<wide|y|~>;\<theta\><rsub|\<star\>>|)>
    <around*|(|<wide|f|~><rsup|<around*|(|2n|)>><around*|(|x;<wide|\<theta\>|~>|)>-<wide|y|~>|)><rsup|2>
  </equation*>

  with <math|r<rsup|<around*|(|2n|)>>=<sqrt|L<rsup|<around*|(|2n|)>><rsub|\<star\>>/L<rsub|0>>>
  and <math|<wide|L|~><rsup|<around*|(|2n|)>><around*|(|0|)>=L<rsub|0>>, we
  have

  <\equation*>
    L<rsup|<around*|(|4n|)>><around*|(|\<theta\><rsub|\<star\>>,<wide|\<theta\>|~><rsub|\<star\>>|)>=<frac|L<rsup|<around*|(|2n|)>><rsub|\<star\>>|L<rsub|0>><wide|L|~><rsup|<around*|(|2n|)>><around*|(|<wide|\<theta\>|~><rsub|\<star\>>|)>=<frac|L<rsup|<around*|(|2n|)>><rsub|\<star\>>|L<rsub|0>>
    <wide|L|~><rsup|<around*|(|2n|)>><rsub|\<star\>>=L<rsup|<around*|(|2n|)>><rsub|\<star\>>
    <frac|<wide|L|~><rsup|<around*|(|2n|)>><rsub|\<star\>>|L<rsub|0>>,
  </equation*>

  Since we have derived that <math|L<rsup|<around*|(|2n|)>><rsub|\<star\>>=L<rsup|<around*|(|n|)>><rsub|\<star\>>
  <around*|(|<wide|L|~><rsup|<around*|(|n|)>><rsub|\<star\>>/L<rsub|0>|)>>,
  we find

  <\equation*>
    L<rsup|<around*|(|4n|)>><rsub|\<star\>>=L<rsup|<around*|(|n|)>><rsub|\<star\>>
    <frac|<wide|L|~><rsup|<around*|(|n|)>><rsub|\<star\>>|L<rsub|0>><frac|<wide|L|~><rsup|<around*|(|2n|)>><rsub|\<star\>>|L<rsub|0>>.
  </equation*>

  Interestingly, we can regard <math|L<rsup|<around*|(|n|)>><rsub|\<star\>>>
  as <math|<wide|L|~><rsub|\<star\>><rsup|<around*|(|0|)>>> by viewing
  <math|f<rsup|<around*|(|n|)>><around*|(|x;\<theta\>|)>> as fitting the
  residual error of <math|f<around*|(|x;0|)>>. Indeed, recall that
  <math|f<around*|(|x;0|)>=0> for all <math|x>, the loss for the residual
  error of <math|f<around*|(|x;0|)>> comes to be

  <\equation*>
    <wide|L|~><rsup|<around*|(|n|)>><around*|(|<wide|\<theta\>|~>|)>\<assign\><big|int>\<mathd\>x
    \<mathd\>y p<around*|(|x,y|)> <around*|(|<wide|f|~><rsup|<around*|(|n|)>><around*|(|x;<wide|\<theta\>|~>|)>-y|)><rsup|2>,
  </equation*>

  which is exactly the form of <math|L<rsup|<around*|(|n|)>>>. It means
  <math|<wide|L|~><rsub|\<star\>><rsup|<around*|(|0|)>>\<assign\>min<rsub|<wide|\<theta\>|~>\<in\>\<bbb-R\><rsup|n>><wide|L|~><rsup|<around*|(|n|)>><around*|(|<wide|\<theta\>|~>|)>=min<rsub|\<theta\>\<in\>\<bbb-R\><rsup|n>>L<rsup|<around*|(|n|)>><around*|(|\<theta\>|)>=L<rsup|<around*|(|n|)>><rsub|\<star\>>>.
  So, we arrive at

  <\equation*>
    <frac|L<rsup|<around*|(|4n|)>><rsub|\<star\>>|L<rsub|0>>=<frac|<wide|L|~><rsup|<around*|(|0|)>><rsub|\<star\>>|L<rsub|0>>
    <frac|<wide|L|~><rsup|<around*|(|n|)>><rsub|\<star\>>|L<rsub|0>><frac|<wide|L|~><rsup|<around*|(|2n|)>><rsub|\<star\>>|L<rsub|0>>.
  </equation*>

  \;

  Generally, we have

  <\equation>
    <frac|L<rsup|<around*|(|2<rsup|m> n|)>><rsub|\<star\>>|L<rsub|0>>=<frac|<wide|L|~><rsup|<around*|(|0|)>><rsub|\<star\>>|L<rsub|0>>
    <frac|<wide|L|~><rsup|<around*|(|n|)>><rsub|\<star\>>|L<rsub|0>><frac|<wide|L|~><rsup|<around*|(|2n|)>><rsub|\<star\>>|L<rsub|0>>\<cdots\><frac|<wide|L|~><rsup|<around*|(|2<rsup|<around*|(|m-1|)>>
    n|)>><rsub|\<star\>>|L<rsub|0>>.<label|equation:L star regression>
  </equation>

  <section|Power-Law>

  <subsection|Power-Law Arises From Self-Similarity of Complexity>

  We wonder when the power-law relation between
  <math|L<rsup|<around*|(|2<rsup|m> n|)>><rsub|\<star\>>/L<rsub|0>> and
  <math|2<rsup|m> n> may arise. It is known that a function
  <math|f<around*|(|x|)>:\<bbb-R\>\<rightarrow\>\<bbb-R\>> is power-law,
  which means there is a <math|\<gamma\>\<in\>\<bbb-R\>> such that
  <math|f<around*|(|x|)>=x<rsup|\<gamma\>>>, if and only if there is a
  function <math|k:\<bbb-R\><rsub|+>\<rightarrow\>\<bbb-R\><rsub|+>> such
  that, for <math|\<forall\>s,x>, <math|f<around*|(|s x|)>=k<around*|(|s|)>
  f<around*|(|x|)>>. Applying to our situation, we find that, given <math|n>,
  the condition for power-law to arise is that
  <math|L<rsup|<around*|(|2<rsup|<around*|(|m+1|)>>
  n|)>><rsub|\<star\>>/L<rsup|<around*|(|2<rsup|m> n|)>><rsub|\<star\>>> is
  independent of <math|m>. From equation (<reference|equation:L star
  regression>), we have

  <\equation*>
    <frac|L<rsup|<around*|(|2<rsup|<around*|(|m+1|)>>
    n|)>><rsub|\<star\>>|L<rsup|<around*|(|2<rsup|m>
    n|)>><rsub|\<star\>>>=<frac|<wide|L|~><rsup|<around*|(|2<rsup|m>
    n|)>><rsub|\<star\>>|L<rsub|0>>,
  </equation*>

  indicating that <math|><math|<wide|L|~><rsup|<around*|(|2<rsup|m>
  n|)>><rsub|\<star\>>> shall be independent of <math|m>. Recall that
  <math|<wide|L|~><rsup|<around*|(|2<rsup|m>
  n|)>><around*|(|<wide|\<theta\>|~>|)>>, as the loss of residual error of
  <math|f<rsup|<around*|(|2<rsup|m> n|)>><around*|(|x;\<theta\>|)>>, has a
  \Pstandard\Q format. Thus, its minimum,
  <math|<wide|L|~><rsup|<around*|(|2<rsup|m> n|)>><rsub|\<star\>>>, is
  completely determined by the complexity of
  <math|<wide|p|~><rsup|<around*|(|2<rsup|m>
  n|)>><around*|(|x,<wide|y|~>;\<theta\><rsub|\<star\>>|)>> where
  <math|\<theta\><rsub|\<star\>>=argmin L<rsup|<around*|(|2<rsup|m> n|)>>>,
  the best-fit of <math|f<rsup|<around*|(|2<rsup|m>
  n|)>><around*|(|x;\<theta\>|)>>. TODO
</body>

<\initial>
  <\collection>
    <associate|page-medium|paper>
  </collection>
</initial>

<\references>
  <\collection>
    <associate|auto-1|<tuple|1|1>>
    <associate|auto-10|<tuple|1.3|4>>
    <associate|auto-11|<tuple|1.3.1|4>>
    <associate|auto-2|<tuple|1.1|1>>
    <associate|auto-3|<tuple|1.1.1|1>>
    <associate|auto-4|<tuple|1.1.2|1>>
    <associate|auto-5|<tuple|1.1.3|1>>
    <associate|auto-6|<tuple|1.2|2>>
    <associate|auto-7|<tuple|1.2.1|2>>
    <associate|auto-8|<tuple|1.2.2|2>>
    <associate|auto-9|<tuple|1.2.3|3>>
    <associate|equation:L star regression|<tuple|1.1|4>>
    <associate|footnote-1.1|<tuple|1.1|2>>
    <associate|footnr-1.1|<tuple|1.1|2>>
  </collection>
</references>

<\auxiliary>
  <\collection>
    <\associate|toc>
      <vspace*|1fn><with|font-series|<quote|bold>|math-font-series|<quote|bold>|1<space|2spc>Scaling
      and Power-Law> <datoms|<macro|x|<repeat|<arg|x>|<with|font-series|medium|<with|font-size|1|<space|0.2fn>.<space|0.2fn>>>>>|<htab|5mm>>
      <no-break><pageref|auto-1><vspace|0.5fn>

      1.1<space|2spc>Complexity <datoms|<macro|x|<repeat|<arg|x>|<with|font-series|medium|<with|font-size|1|<space|0.2fn>.<space|0.2fn>>>>>|<htab|5mm>>
      <no-break><pageref|auto-2>

      <with|par-left|<quote|1tab>|1.1.1<space|2spc>Distribution Is a Data
      Generator <datoms|<macro|x|<repeat|<arg|x>|<with|font-series|medium|<with|font-size|1|<space|0.2fn>.<space|0.2fn>>>>>|<htab|5mm>>
      <no-break><pageref|auto-3>>

      <with|par-left|<quote|1tab>|1.1.2<space|2spc>Model Is a Functional Form
      <datoms|<macro|x|<repeat|<arg|x>|<with|font-series|medium|<with|font-size|1|<space|0.2fn>.<space|0.2fn>>>>>|<htab|5mm>>
      <no-break><pageref|auto-4>>

      <with|par-left|<quote|1tab>|1.1.3<space|2spc>Complexity of Dataset
      Examed by Model <datoms|<macro|x|<repeat|<arg|x>|<with|font-series|medium|<with|font-size|1|<space|0.2fn>.<space|0.2fn>>>>>|<htab|5mm>>
      <no-break><pageref|auto-5>>

      1.2<space|2spc>Relation between Minimum of Loss and Number of
      Parameters: A Simple Example <datoms|<macro|x|<repeat|<arg|x>|<with|font-series|medium|<with|font-size|1|<space|0.2fn>.<space|0.2fn>>>>>|<htab|5mm>>
      <no-break><pageref|auto-6>

      <with|par-left|<quote|1tab>|1.2.1<space|2spc>Notations
      <datoms|<macro|x|<repeat|<arg|x>|<with|font-series|medium|<with|font-size|1|<space|0.2fn>.<space|0.2fn>>>>>|<htab|5mm>>
      <no-break><pageref|auto-7>>

      <with|par-left|<quote|1tab>|1.2.2<space|2spc>An Extending-Rescaling
      Process of Loss Function <datoms|<macro|x|<repeat|<arg|x>|<with|font-series|medium|<with|font-size|1|<space|0.2fn>.<space|0.2fn>>>>>|<htab|5mm>>
      <no-break><pageref|auto-8>>

      <with|par-left|<quote|1tab>|1.2.3<space|2spc>Generalizing to More
      Parameters <datoms|<macro|x|<repeat|<arg|x>|<with|font-series|medium|<with|font-size|1|<space|0.2fn>.<space|0.2fn>>>>>|<htab|5mm>>
      <no-break><pageref|auto-9>>

      1.3<space|2spc>Power-Law <datoms|<macro|x|<repeat|<arg|x>|<with|font-series|medium|<with|font-size|1|<space|0.2fn>.<space|0.2fn>>>>>|<htab|5mm>>
      <no-break><pageref|auto-10>

      <with|par-left|<quote|1tab>|1.3.1<space|2spc>Power-Law Arises From
      Self-Similarity of Complexity <datoms|<macro|x|<repeat|<arg|x>|<with|font-series|medium|<with|font-size|1|<space|0.2fn>.<space|0.2fn>>>>>|<htab|5mm>>
      <no-break><pageref|auto-11>>
    </associate>
  </collection>
</auxiliary>