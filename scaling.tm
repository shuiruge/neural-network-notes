<TeXmacs|2.1>

<style|book>

<\body>
  <chapter|Scaling and Power Law>

  <section|Power Law>

  <subsection|Distribution Is Data Generator>

  Ideally, a dataset is described by a distribution. This distribution is in
  fact a data generator. For example, Internet is a generator of texture
  data. Expression <math|x\<sim\>P<rsub|<text|Internet>>> means the text
  <math|x> is generated by Internet. We cannot say how many texture data are
  there on the Internet, since Internet can generate a new one at any time.
  Thus, <math|P<rsub|<text|Internet>>> is really a data generator; and we can
  generate infinite data from <math|P<rsub|<text|Internet>>>, theoretically.

  <subsection|Model Is Functional Form>

  What a model represents is the functional form. When we define a model,
  such as a neural network, we have to figure out how the model varies when
  more parameters are added. Ideally, we can think of a model defined with
  infinitly many parameters, with all but finite number of them vanishing.
  For example, we define a model <math|f<around*|(|x;\<theta\>|)>> to be
  polynomial with <math|\<theta\>> the collection of coefficients. Thus,
  <math|f<around*|(|x;\<theta\>|)>=<big|sum><rsub|k=1><rsup|+\<infty\>>\<theta\><rsub|k>
  x<rsup|k>> where only finite number of <math|\<theta\><rsub|k>> are not
  zero. What we have defined is the functional form of <math|f>.

  <subsection|Complexity of Dataset Characterized by Model>

  Let us consider a simple one-dimensional regression task. Given by a
  dataset distribution <math|P> and the form of model <math|f>, the loss
  function is

  <\equation*>
    L<around*|(|\<theta\>|)>=<big|int>\<mathd\>x\<mathd\>y
    \ p<around*|(|x,y|)><around*|(|f<around*|(|x;\<theta\>|)>-y|)><rsup|2>.
  </equation*>

  Without losing generality, suppose that <math|f<around*|(|x;0|)>=0> for
  <math|\<forall\>x\<in\>\<bbb-R\>>. Thus,
  <math|L<rsub|0>\<assign\>L<around*|(|0|)>=\<bbb-E\><rsub|<around*|(|x,y|)>\<sim\>P><around*|[|y<rsup|2>|]>>.
  By optimization, we have <math|\<theta\><rsub|\<star\>>\<assign\>argmin<rsub|\<theta\>\<in\>\<bbb-R\><rsup|n>>L<around*|(|\<theta\>|)>>
  and <math|L<rsub|\<star\>>\<assign\>L<around*|(|\<theta\><rsub|\<star\>>|)>>.

  Given the model <math|f>, the relation between <math|n> and
  <math|L<rsub|\<star\>>> reflects the complexity of dataset <math|P>. The
  more complex the <math|P> is, the greater <math|n> it is needed for
  obtaining a fixed <math|L<rsub|\<star\>>>, or the greater
  <math|L<rsub|\<star\>>> is obtained with a fixed number of parameters.
  Contrarily, if the <math|P> is simpler, the smaller <math|n> is sufficient
  for reaching a fixed <math|L<rsub|\<star\>>>, or obtaining a smaller
  <math|L<rsub|\<star\>>> when <math|n> is fixed.

  <subsection|Example>

  We are to exame how the <math|L<rsub|\<star\>>> changes with <math|n>
  increasing. To do so, we have to employ proper notations. A model with
  <math|n> parameters is denoted by <math|f<rsup|<around*|(|n|)>><around*|(|x;\<theta\><rsup|<around*|(|n|)>>|)>>.
  Usually, we omit the <math|<around*|(|n|)>> in
  <math|\<theta\><rsup|<around*|(|n|)>>>, thus
  <math|f<rsup|<around*|(|n|)>><around*|(|x;\<theta\>|)>>, since it is
  obvious that <math|\<theta\>\<in\>\<bbb-R\><rsup|n>>. Its loss is denoted
  by <math|L<rsup|<around*|(|n|)>><around*|(|\<theta\><rsup|<around*|(|n|)>>|)>>
  or <math|L<rsup|<around*|(|n|)>><around*|(|\<theta\>|)>>, thus
  <math|L<rsup|<around*|(|n|)>><rsub|\<star\>>> and
  <math|\<theta\><rsup|<around*|(|n|)>><rsub|\<star\>>> for its <math|min>
  and <math|argmin> respectively.

  With these new notations, we enlarge the space of parameters by introducing
  a new model <math|f<rsup|<around*|(|2n|)>>> and replacing
  <math|f<rsup|<around*|(|n|)>><around*|(|x;\<theta\>|)>\<rightarrow\>f<rsup|<around*|(|n|)>><around*|(|x;\<theta\>|)>+r
  <wide|f|~><rsup|<around*|(|n|)>><around*|(|x;<wide|\<theta\>|~>|)>>, where
  <math|r\<in\><around*|(|0,+\<infty\>|)>> to be determined and
  <math|<wide|f|~>> a new model with <math|n> parameters. In this way, the
  new model has <math|2n> parameters. For this new model, the loss becomes

  <\align>
    <tformat|<table|<row|<cell|L<rsup|<around*|(|2n|)>><around*|(|\<theta\>,<wide|\<theta\>|~>|)>\<assign\>>|<cell|<big|int>\<mathd\>x\<mathd\>y
    \ p<around*|(|x,y|)><around*|(|f<rsup|<around*|(|n|)>><around*|(|x;\<theta\>|)>+r
    <wide|f|~><rsup|<around*|(|n|)>><around*|(|x;<wide|\<theta\>|~>|)>-y|)><rsup|2>>>|<row|<cell|<around*|{|<around*|[|\<cdots\>|]>=1|}>=>|<cell|<big|int>\<mathd\>x\<mathd\>y
    \ p<around*|(|x,y|)><around*|[|r<big|int>\<mathd\><wide|y|~>
    \<delta\><around*|(|r <wide|y|~>-y+f<rsup|<around*|(|n|)>><around*|(|x;\<theta\>|)>|)>|]><around*|(|f<rsup|<around*|(|n|)>><around*|(|x;\<theta\>|)>+r
    <wide|f|~><rsup|<around*|(|n|)>><around*|(|x;<wide|\<theta\>|~>|)>-y|)><rsup|2>>>|<row|<cell|=>|<cell|<big|int>\<mathd\>x\<mathd\><wide|y|~><around*|[|r<big|int>\<mathd\>y
    \ p<around*|(|x,y|)> \<delta\><around*|(|r
    <wide|y|~>-y+f<rsup|<around*|(|n|)>><around*|(|x;\<theta\>|)>|)>|]><around*|(|r
    <wide|f|~><rsup|<around*|(|n|)>><around*|(|x;<wide|\<theta\>|~>|)>-r
    <wide|y|~>|)><rsup|2>.>>>>
  </align>

  Define

  <\equation*>
    <wide|p|~><rsup|<around*|(|n|)>><around*|(|x,<wide|y|~>;\<theta\>|)>\<assign\>r<big|int>\<mathd\>y
    \ p<around*|(|x,y|)> \<delta\><around*|(|r
    <wide|y|~>-y+f<rsup|<around*|(|n|)>><around*|(|x;\<theta\>|)>|)>.
  </equation*>

  Thus, we have <\footnote>
    We have, inversely from the conclusion

    <\align>
      <tformat|<table|<row|<cell|>|<cell|<big|int>\<mathd\>x\<mathd\>y<rprime|'>
      p<rprime|'><rsub|D><around*|(|x,y<rprime|'>|)><around*|(|r
      f<rprime|'><around*|(|x;\<theta\><rprime|'>|)>-r
      y<rprime|'>|)><rsup|2>>>|<row|<cell|<around*|{|p<rprime|'><rsub|D>\<assign\>\<cdots\>|}>=>|<cell|<big|int>\<mathd\>x\<mathd\>y<rprime|'>
      <around*|[|r<big|int>\<mathd\>y p<rsub|D><around*|(|x,y|)>
      \<delta\><around*|(|r y<rprime|'>-y+f<around*|(|x;\<theta\>|)>|)>|]>
      <around*|(|r f<rprime|'><around*|(|x;\<theta\><rprime|'>|)>-r
      y<rprime|'>|)><rsup|2>>>|<row|<cell|=>|<cell|<big|int>\<mathd\>x\<mathd\>y
      p<rsub|D><around*|(|x,y|)> <around*|[|r<big|int>\<mathd\>y<rprime|'>
      \<delta\><around*|(|r y<rprime|'>-y+f<around*|(|x;\<theta\>|)>|)>|]>
      <around*|(|r f<rprime|'><around*|(|x;\<theta\><rprime|'>|)>-r
      y<rprime|'>|)><rsup|2>>>|<row|<cell|<around*|{|<text|integrate over
      <math|y<rprime|'>>>|}>=>|<cell|<big|int>\<mathd\>x\<mathd\>y
      p<rsub|D><around*|(|x,y|)> <around*|(|f<around*|(|x;\<theta\>|)>+r
      f<rprime|'><around*|(|x;\<theta\><rprime|'>|)>-y|)><rsup|2>,>>>>
    </align>

    which is exactly the condition.
  </footnote>

  <\equation*>
    L<rsup|<around*|(|2n|)>><around*|(|\<theta\>,<wide|\<theta\>|~>|)>=r<rsup|2>
    <big|int>\<mathd\>x \<mathd\><wide|y|~>
    <wide|p|~><rsup|<around*|(|n|)>><around*|(|x,<wide|y|~>;\<theta\>|)>
    <around*|(|<wide|f|~><rsup|<around*|(|n|)>><around*|(|x;<wide|\<theta\>|~>|)>-<wide|y|~>|)><rsup|2>.
  </equation*>

  The <math|<wide|y|~>> is the rescaled residual error, and
  <math|<wide|p|~><rsup|<around*|(|n|)>>> describes its
  distribution.<\footnote>
    We have to show that <math|<wide|p|~><rsup|<around*|(|n|)>><around*|(|\<ldots\>;\<theta\>|)>>
    is indeed a distribution, that is, for any <math|\<theta\>>,
    <math|<big|int>\<mathd\>x\<mathd\><wide|y|~>
    <wide|p|~><rsup|<around*|(|n|)>><around*|(|x,<wide|y|~>;\<theta\>|)>=1>.
    Indeed,

    <\align>
      <tformat|<table|<row|<cell|>|<cell|<big|int>\<mathd\>x\<mathd\><wide|y|~>
      <wide|p|~><rsup|<around*|(|n|)>><around*|(|x,<wide|y|~>;\<theta\>|)>>>|<row|<cell|<around*|{|<wide|p|~><rsup|<around*|(|n|)>>\<assign\>\<cdots\>|}>=>|<cell|<big|int>\<mathd\>x\<mathd\>y<rprime|'>
      <around*|[|r<big|int>\<mathd\>y \ p<around*|(|x,y|)>
      \<delta\><around*|(|r <wide|y|~>-y+f<around*|(|x;\<theta\>|)>|)>|]>>>|<row|<cell|=>|<cell|<big|int>\<mathd\>x
      \<mathd\>y p<around*|(|x,y|)> <around*|[|r<big|int>\<mathd\><wide|y|~>
      \<delta\><around*|(|r <wide|y|~>-y+f<around*|(|x;\<theta\>|)>|)>|]>>>|<row|<cell|<around*|{|<text|integrate
      over <math|<wide|y|~>>>|}>=>|<cell|<big|int>\<mathd\>x \<mathd\>y
      p<around*|(|x,y|)>>>|<row|<cell|<around*|{|<text|<math|p<rsub|D>><math|
      is a distribution>>|}>=>|<cell|1.>>>>
    </align>
  </footnote>

  \;

  If we freeze the <math|\<theta\>> to be <math|\<theta\><rsub|\<star\>>> and
  only adjust the <math|\<varphi\>> while optimizing the model, then
  <math|<wide|p|~><rsup|<around*|(|n|)>>> depends only on <math|x> and
  <math|<wide|y|~>>, and <math|L<rsup|<around*|(|2n|)>>> depends only on
  <math|<wide|\<theta\>|~>>. Since <math|L<rsup|<around*|(|2n|)>><around*|(|\<theta\><rsub|\<star\>>,0|)>=L<rsub|\<star\>><rsup|<around*|(|n|)>>>,
  for formally going back to the original, let
  <math|r=<sqrt|L<rsub|\<star\>><rsup|<around*|(|n|)>>/L<around*|(|0|)>>>,
  thus

  <\equation*>
    <wide|L|~><rsup|<around*|(|n|)>><around*|(|<wide|\<theta\>|~>|)>\<assign\><frac|L<around*|(|0|)>|L<rsub|\<star\>><rsup|<around*|(|n|)>>>L<rsup|<around*|(|2n|)>><around*|(|\<theta\><rsub|\<star\>>,<wide|\<theta\>|~>|)>=<big|int>\<mathd\>x
    \<mathd\><wide|y|~> <wide|p|~><rsup|<around*|(|n|)>><around*|(|x,<wide|y|~>;\<theta\><rsub|\<star\>>|)>
    <around*|(|<wide|f|~><rsup|<around*|(|n|)>><around*|(|x;<wide|\<theta\>|~>|)>-<wide|y|~>|)><rsup|2>.
  </equation*>

  Comparing with the expression of <math|L<rsup|<around*|(|n|)>>>,
  <math|<wide|L|~><rsup|<around*|(|n|)>>> is formally the same as <math|L>,
  even with <math|<wide|L|~><rsup|<around*|(|n|)>><around*|(|0|)>=L<rsup|<around*|(|n|)>><around*|(|0|)>=L<rsub|0>>.
  The <math|<wide|f|~>> is equivalent to <math|f> since their spaces of
  parameters are both <math|n>-dimensional. Thus, the only difference is that
  <math|p> is replaced by <math|<wide|p|~><rsup|<around*|(|n|)>><around*|(|\<ldots\>;\<theta\><rsub|\<star\>>|)>>.

  The minimal value of <math|<wide|L|~><rsup|<around*|(|n|)>>> depends on the
  complexity of <math|<wide|P|~><rsup|<around*|(|n|)>><around*|(|\<theta\><rsub|\<star\>>|)>>,
  which characterizes the distribution of residual errors of
  <math|f<rsup|<around*|(|n|)>><around*|(|\<ldots\>;\<theta\><rsub|\<star\>>|)>>.
  Denoting <math|<wide|\<theta\>|~><rsub|\<star\>>\<assign\>argmin<rsub|<wide|\<theta\>|~>\<in\>\<bbb-R\><rsup|n>><wide|L|~><rsup|<around*|(|n|)>><around*|(|<wide|\<theta\>|~>|)>>
  and <math|<wide|L|~><rsub|\<star\>><rsup|<around*|(|n|)>>\<assign\><wide|L|~><rsup|<around*|(|n|)>><around*|(|<wide|\<theta\>|~><rsub|\<star\>>|)>>,
  we have

  <\equation*>
    L<rsup|<around*|(|2n|)>><rsub|\<star\>>\<assign\>L<rsup|<around*|(|2n|)>><around*|(|\<theta\><rsub|\<star\>>,<wide|\<theta\>|~><rsub|\<star\>>|)>=<frac|L<rsup|<around*|(|n|)>><rsub|\<star\>>|L<rsub|0>><wide|L|~><rsup|<around*|(|n|)>><around*|(|<wide|\<theta\>|~><rsub|\<star\>>|)>=<frac|L<rsup|<around*|(|n|)>><rsub|\<star\>>|L<rsub|0>>
    <wide|L|~><rsub|\<star\>><rsup|<around*|(|n|)>>=L<rsup|<around*|(|n|)>><rsub|\<star\>>
    <frac|<wide|L|~><rsub|\<star\>><rsup|<around*|(|n|)>>|L<rsub|0>>.
  </equation*>

  Even though <math|L<rsup|<around*|(|2n|)>><rsub|\<star\>>> or
  <math|L<rsup|<around*|(|2n|)>><around*|(|\<theta\><rsub|\<star\>>,<wide|\<theta\>|~><rsub|\<star\>>|)>>
  may not be the <math|min<rsub|<around*|(|\<theta\>,<wide|\<theta\>|~>|)>\<in\>\<bbb-R\><rsup|2n>>L<rsup|<around*|(|2n|)>><around*|(|\<theta\>,<wide|\<theta\>|~>|)>>,
  we suppose that the difference is negligible. That is, we suppose that the
  optimization by first on <math|\<theta\>> and then on
  <math|<wide|\<theta\>|~>> with <math|\<theta\>> frozen can approximate the
  best-fit, by which the <math|\<theta\>> and <math|<wide|\<theta\>|~>> are
  optimized synchronously. The reason is that the optimization of
  <math|\<theta\>> has taken most of the work that minimizes the loss, while
  optimizing <math|<wide|\<theta\>|~>> in the next step is just fine-tuning.

  <subsection|Generalization to Higher Dimensions>

  Now, we consider <math|4n>-dimensional space of parameters. To do this, we
  consider <math|f<rsup|<around*|(|n|)>><around*|(|x;\<theta\><rsup|<around*|(|n|)>>|)>+<wide|f|~><rsup|<around*|(|n|)>><around*|(|x;<wide|\<theta\>|~><rsup|<around*|(|n|)>>|)>>
  as the baseline model, <math|f<rsup|<around*|(|2n|)>><around*|(|x;\<theta\>|)>\<assign\>f<rsup|<around*|(|n|)>><around*|(|x;\<theta\><rsup|<around*|(|n|)>>|)>+<wide|f|~><around*|(|x;<wide|\<theta\>|~><rsup|<around*|(|n|)>>|)>>
  and <math|\<theta\>\<assign\><around*|(|\<theta\><rsup|<around*|(|n|)>>,<wide|\<theta\>|~><rsup|<around*|(|n|)>>|)>\<in\>\<bbb-R\><rsup|2n>>.
  Thus, from what we have derived, <math|L<rsup|<around*|(|2n|)>><rsub|\<star\>>=L<rsup|<around*|(|n|)>><rsub|\<star\>>
  <around*|(|<wide|L|~><rsup|<around*|(|n|)>><rsub|\<star\>>/L<rsub|0>|)>>.

  Next, we consider the residual error. As before, replace
  <math|f<rsup|<around*|(|2n|)>><around*|(|x;\<theta\>|)>> by
  <math|f<rsup|<around*|(|2n|)>><around*|(|x;\<theta\>|)>+r
  <wide|f|~><rsup|<around*|(|2n|)>><around*|(|x;<wide|\<theta\>|~>|)>> where
  <math|r\<in\><around*|(|0,+\<infty\>|)>> to be determined. Thus the loss
  <math|L<rsup|<around*|(|4n|)>><around*|(|\<theta\>,<wide|\<theta\>|~>|)>\<assign\><big|int>\<mathd\>x\<mathd\>y
  \ p<around*|(|x,y|)><around*|(|f<rsup|<around*|(|2n|)>><around*|(|x;\<theta\>|)>+r
  <wide|f|~><rsup|<around*|(|2n|)>><around*|(|x;<wide|\<theta\>|~>|)>-y|)><rsup|2>>.
  Following the same steps, we arrive at

  <\equation*>
    <wide|p|~><rsup|<around*|(|2n|)>><around*|(|x,<wide|y|~>;\<theta\>|)>\<assign\>r<big|int>\<mathd\>y
    \ p<around*|(|x,y|)> \<delta\><around*|(|r
    <wide|y|~>-y+f<rsup|<around*|(|2n|)>><around*|(|x;\<theta\>|)>|)>
  </equation*>

  which characterizes the distribution of the (rescaled) residual error of
  <math|f<rsup|<around*|(|2n|)>><around*|(|x;\<theta\>|)>>, and

  <\equation*>
    L<rsup|<around*|(|4n|)>><around*|(|\<theta\>,<wide|\<theta\>|~>|)>=r<rsup|2>
    <big|int>\<mathd\>x \<mathd\><wide|y|~>
    <wide|p|~><rsup|<around*|(|2n|)>><around*|(|x,<wide|y|~>;\<theta\>|)>
    <around*|(|<wide|f|~><rsup|<around*|(|2n|)>><around*|(|x;<wide|\<theta\>|~>|)>-<wide|y|~>|)><rsup|2>.
  </equation*>

  Again, by denoting

  \;

  <\equation*>
    <wide|L|~><rsup|<around*|(|2n|)>><around*|(|<wide|\<theta\>|~>|)>\<assign\><frac|L<rsub|0>|L<rsup|<around*|(|2n|)>><rsub|\<star\>>>L<rsup|<around*|(|4n|)>><around*|(|\<theta\><rsub|\<star\>>,<wide|\<theta\>|~>|)>
  </equation*>

  with <math|r=<sqrt|L<rsup|<around*|(|2n|)>><rsub|\<star\>>/L<rsub|0>>> and
  <math|<wide|L|~><rsup|<around*|(|2n|)>><around*|(|0|)>=L<rsub|0>>, we have

  <\equation*>
    L<rsup|<around*|(|4n|)>><around*|(|\<theta\><rsub|\<star\>>,<wide|\<theta\>|~><rsub|\<star\>>|)>=<frac|L<rsup|<around*|(|2n|)>><rsub|\<star\>>|L<rsub|0>><wide|L|~><rsup|<around*|(|2n|)>><around*|(|<wide|\<theta\>|~><rsub|\<star\>>|)>=<frac|L<rsup|<around*|(|2n|)>><rsub|\<star\>>|L<rsub|0>>
    <wide|L|~><rsup|<around*|(|2n|)>><rsub|\<star\>>=L<rsup|<around*|(|2n|)>><rsub|\<star\>>
    <frac|<wide|L|~><rsup|<around*|(|2n|)>><rsub|\<star\>>|L<rsub|0>>,
  </equation*>

  Since we have derived that <math|L<rsup|<around*|(|2n|)>><rsub|\<star\>>=L<rsup|<around*|(|n|)>><rsub|\<star\>>
  <around*|(|<wide|L|~><rsup|<around*|(|n|)>><rsub|\<star\>>/L<rsub|0>|)>>,
  we find

  <\equation*>
    L<rsup|<around*|(|4n|)>><rsub|\<star\>>=L<rsup|<around*|(|n|)>><rsub|\<star\>>
    <frac|<wide|L|~><rsup|<around*|(|n|)>><rsub|\<star\>>|L<rsub|0>><frac|<wide|L|~><rsup|<around*|(|2n|)>><rsub|\<star\>>|L<rsub|0>>.
  </equation*>

  Generally, we have

  <\equation*>
    L<rsup|<around*|(|2<rsup|m> n|)>><rsub|\<star\>>=L<rsup|<around*|(|n|)>><rsub|\<star\>>
    <frac|<wide|L|~><rsup|<around*|(|n|)>><rsub|\<star\>>|L<rsub|0>><frac|<wide|L|~><rsup|<around*|(|2n|)>><rsub|\<star\>>|L<rsub|0>>\<cdots\><frac|<wide|L|~><rsup|<around*|(|2<rsup|<around*|(|m-1|)>>
    n|)>><rsub|\<star\>>|L<rsub|0>>.
  </equation*>
</body>

<\initial>
  <\collection>
    <associate|page-medium|paper>
  </collection>
</initial>

<\references>
  <\collection>
    <associate|auto-1|<tuple|1|1>>
    <associate|auto-2|<tuple|1.1|1>>
    <associate|auto-3|<tuple|1.1.1|?>>
    <associate|auto-4|<tuple|1.1.2|?>>
    <associate|auto-5|<tuple|1.1.3|?>>
    <associate|auto-6|<tuple|1.1.4|?>>
    <associate|auto-7|<tuple|1.1.5|?>>
    <associate|auto-8|<tuple|1.1.6|?>>
    <associate|footnote-1|<tuple|1|?>>
    <associate|footnote-1.1|<tuple|1.1|1>>
    <associate|footnote-1.2|<tuple|1.2|?>>
    <associate|footnote-1.3|<tuple|1.3|?>>
    <associate|footnr-1.1|<tuple|1.1|1>>
    <associate|footnr-1.2|<tuple|1.2|?>>
    <associate|footnr-1.3|<tuple|1.3|?>>
  </collection>
</references>

<\auxiliary>
  <\collection>
    <\associate|toc>
      <vspace*|1fn><with|font-series|<quote|bold>|math-font-series|<quote|bold>|1<space|2spc>Scaling
      and Power Law> <datoms|<macro|x|<repeat|<arg|x>|<with|font-series|medium|<with|font-size|1|<space|0.2fn>.<space|0.2fn>>>>>|<htab|5mm>>
      <no-break><pageref|auto-1><vspace|0.5fn>

      1.1<space|2spc>Power Law <datoms|<macro|x|<repeat|<arg|x>|<with|font-series|medium|<with|font-size|1|<space|0.2fn>.<space|0.2fn>>>>>|<htab|5mm>>
      <no-break><pageref|auto-2>
    </associate>
  </collection>
</auxiliary>