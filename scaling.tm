<TeXmacs|2.1>

<style|book>

<\body>
  <chapter|Scaling and Power Law>

  <section|Power Law>

  <subsection|Distribution Is Data Generator>

  Ideally, a dataset is described by a distribution. This distribution is in
  fact a data generator. For example, Internet is a generator of texture
  data. Expression <math|x\<sim\>P<rsub|<text|Internet>>> means the text
  <math|x> is generated by Internet. We cannot say how many texture data are
  there on the Internet, since Internet can generate a new one at any time.
  Thus, <math|P<rsub|<text|Internet>>> is really a data generator.

  <subsection|Model Is Functional Form>

  What a model represents is the functional form. When we define a model,
  such as a neural network, we have to figure out how the model varies when
  more parameters are added. Ideally, we can think of a model defined with
  infinitly many parameters, with all but finite number of them vanishing.
  For example, we define a model <math|f<around*|(|x;\<theta\>|)>> to be
  polynomial with <math|\<theta\>> the collection of coefficients. Thus,
  <math|f<around*|(|x;\<theta\>|)>=<big|sum><rsub|k=1><rsup|+\<infty\>>\<theta\><rsub|k>
  x<rsup|k>> where only finite number of <math|\<theta\><rsub|k>> are not
  zero. What we have defined is the functional form of <math|f>.

  <subsection|Complexity of Dataset Characterized by Model>

  Let us consider a simple one-dimensional regression task. Given by a
  dataset distribution <math|P<rsub|D>> and the form of model <math|f>, the
  loss function is

  <\equation*>
    L<around*|(|\<theta\>|)>=<big|int>\<mathd\>x\<mathd\>y
    \ p<rsub|D><around*|(|x,y|)><around*|(|f<around*|(|x;\<theta\>|)>-y|)><rsup|2>.
  </equation*>

  Without losing generality, suppose that <math|f<around*|(|x;0|)>=0> for
  <math|\<forall\>x\<in\>\<bbb-R\>>. Thus,
  <math|L<around*|(|0|)>=\<bbb-E\><rsub|<around*|(|x,y|)>\<sim\>P<rsub|D>><around*|[|y<rsup|2>|]>>.
  By optimization, we have <math|\<theta\><rsub|\<star\>>\<assign\>argmin<rsub|\<theta\>\<in\>\<bbb-R\><rsup|n>>L<around*|(|\<theta\>|)>>
  and <math|L<rsub|min>\<assign\>L<around*|(|\<theta\><rsub|\<star\>>|)>>.

  Given the model <math|f>, the relation between <math|n> and
  <math|L<rsub|min>> reflects the complexity of dataset <math|P<rsub|D>>. The
  more complex the <math|P<rsub|D>> is, the greater <math|n> it is needed for
  obtaining a fixed <math|L<rsub|min>>, or the greater <math|L<rsub|min>> is
  obtained with a fixed number of parameters. Contrarily, if the
  <math|P<rsub|D>> is simpler, the smaller <math|n> is sufficient for
  reaching a fixed <math|L<rsub|min>>, or obtaining a smaller
  <math|L<rsub|min>> when <math|n> is fixed.

  <subsection|Example>

  We are to exame how the <math|L<rsub|min>> changes with <math|n>
  increasing. For example, we enlarge the space of parameters by introducing
  a new model <math|f<rprime|'>> and replacing
  <math|f<around*|(|x;\<theta\>|)>\<rightarrow\>f<around*|(|x;\<theta\>|)>+r
  f<rprime|'><around*|(|x;\<theta\><rprime|'>|)>>, where
  <math|\<theta\><rprime|'>\<in\>\<bbb-R\><rsup|n>> and
  <math|r\<in\><around*|(|0,+\<infty\>|)>> to be determined. In this way, the
  new model has <math|2n> parameters. For this new model, the loss becomes

  <\align>
    <tformat|<table|<row|<cell|L<around*|(|\<theta\>,\<theta\><rprime|'>|)>\<assign\>>|<cell|<big|int>\<mathd\>x\<mathd\>y
    \ p<rsub|D><around*|(|x,y|)><around*|(|f<around*|(|x;\<theta\>|)>+r
    f<rprime|'><around*|(|x;\<theta\><rprime|'>|)>-y|)><rsup|2>>>|<row|<cell|<around*|{|<around*|[|\<cdots\>|]>=1|}>=>|<cell|<big|int>\<mathd\>x\<mathd\>y
    \ p<rsub|D><around*|(|x,y|)><around*|[|r<big|int>\<mathd\>y<rprime|'>
    \<delta\><around*|(|r y<rprime|'>-y+f<around*|(|x;\<theta\>|)>|)>|]><around*|(|f<around*|(|x;\<theta\>|)>+r
    f<rprime|'><around*|(|x;\<theta\><rprime|'>|)>-y|)><rsup|2>>>|<row|<cell|=>|<cell|<big|int>\<mathd\>x\<mathd\>y<rprime|'><around*|[|r<big|int>\<mathd\>y
    \ p<rsub|D><around*|(|x,y|)> \<delta\><around*|(|r
    y<rprime|'>-y+f<around*|(|x;\<theta\>|)>|)>|]><around*|(|r
    f<rprime|'><around*|(|x;\<theta\><rprime|'>|)>-r
    y<rprime|'>|)><rsup|2>.>>>>
  </align>

  Define

  <\equation*>
    p<rprime|'><rsub|D><around*|(|x,y<rprime|'>;\<theta\>|)>\<assign\>r<big|int>\<mathd\>y
    \ p<rsub|D><around*|(|x,y|)> \<delta\><around*|(|r
    y<rprime|'>-y+f<around*|(|x;\<theta\>|)>|)>.
  </equation*>

  Thus, we have <\footnote>
    We have, inversely from the conclusion

    <\align>
      <tformat|<table|<row|<cell|>|<cell|<big|int>\<mathd\>x\<mathd\>y<rprime|'>
      p<rprime|'><rsub|D><around*|(|x,y<rprime|'>|)><around*|(|r
      f<rprime|'><around*|(|x;\<theta\><rprime|'>|)>-r
      y<rprime|'>|)><rsup|2>>>|<row|<cell|<around*|{|p<rprime|'><rsub|D>\<assign\>\<cdots\>|}>=>|<cell|<big|int>\<mathd\>x\<mathd\>y<rprime|'>
      <around*|[|r<big|int>\<mathd\>y p<rsub|D><around*|(|x,y|)>
      \<delta\><around*|(|r y<rprime|'>-y+f<around*|(|x;\<theta\>|)>|)>|]>
      <around*|(|r f<rprime|'><around*|(|x;\<theta\><rprime|'>|)>-r
      y<rprime|'>|)><rsup|2>>>|<row|<cell|=>|<cell|<big|int>\<mathd\>x\<mathd\>y
      p<rsub|D><around*|(|x,y|)> <around*|[|r<big|int>\<mathd\>y<rprime|'>
      \<delta\><around*|(|r y<rprime|'>-y+f<around*|(|x;\<theta\>|)>|)>|]>
      <around*|(|r f<rprime|'><around*|(|x;\<theta\><rprime|'>|)>-r
      y<rprime|'>|)><rsup|2>>>|<row|<cell|<around*|{|<text|integrate over
      <math|y<rprime|'>>>|}>=>|<cell|<big|int>\<mathd\>x\<mathd\>y
      p<rsub|D><around*|(|x,y|)> <around*|(|f<around*|(|x;\<theta\>|)>+r
      f<rprime|'><around*|(|x;\<theta\><rprime|'>|)>-y|)><rsup|2>,>>>>
    </align>

    which is exactly the condition.
  </footnote>

  <\equation*>
    L<around*|(|\<theta\>,\<theta\><rprime|'>|)>=r<rsup|2>
    <big|int>\<mathd\>x \<mathd\>y p<rprime|'><rsub|D><around*|(|x,y<rprime|'>;\<theta\>|)>
    <around*|(|f<rprime|'><around*|(|x;\<theta\><rprime|'>|)>-y<rprime|'>|)><rsup|2>.
  </equation*>

  In addition, we have to show that <math|p<rprime|'><rsub|D><around*|(|\<ldots\>;\<theta\>|)>>
  is a distribution, that is, for any <math|\<theta\>>,
  <math|<big|int>\<mathd\>x\<mathd\>y<rprime|'>
  p<rprime|'><rsub|D><around*|(|x,y<rprime|'>;\<theta\>|)>=1>. Indeed,

  <\align>
    <tformat|<table|<row|<cell|>|<cell|<big|int>\<mathd\>x\<mathd\>y<rprime|'>
    p<rprime|'><rsub|D><around*|(|x,y<rprime|'>;\<theta\>|)>>>|<row|<cell|<around*|{|p<rprime|'><rsub|D>\<assign\>\<cdots\>|}>=>|<cell|<big|int>\<mathd\>x\<mathd\>y<rprime|'>
    <around*|[|r<big|int>\<mathd\>y \ p<rsub|D><around*|(|x,y|)>
    \<delta\><around*|(|r y<rprime|'>-y+f<around*|(|x;\<theta\>|)>|)>|]>>>|<row|<cell|=>|<cell|<big|int>\<mathd\>x
    \<mathd\>y p<rsub|D><around*|(|x,y|)>
    <around*|[|r<big|int>\<mathd\>y<rprime|'> \<delta\><around*|(|r
    y<rprime|'>-y+f<around*|(|x;\<theta\>|)>|)>|]>>>|<row|<cell|<around*|{|<text|integrate
    over <math|y<rprime|'>>>|}>=>|<cell|<big|int>\<mathd\>x \<mathd\>y
    p<rsub|D><around*|(|x,y|)>>>|<row|<cell|<around*|{|<text|<math|p<rsub|D>><math|
    is a distribution>>|}>=>|<cell|1.>>>>
  </align>

  \;

  If we freeze the <math|\<theta\>> to be <math|\<theta\><rsub|\<star\>>> and
  only adjust the <math|\<theta\><rprime|'>> while optimizing the model, then
  <math|p<rprime|'><rsub|D>> depends only on <math|x> and <math|y<rprime|'>>,
  and <math|L> depends only on <math|\<theta\><rprime|'>>. In this situation,
  the <math|y<rprime|'>> becomes the residual error of
  <math|f<around*|(|x;\<theta\><rsub|\<star\>>|)>>. Since
  <math|L<around*|(|\<theta\><rsub|\<star\>>,0|)>=L<rsub|min>>, for formally
  going back to the original, let <math|r=<sqrt|L<rsub|min>/L<around*|(|0|)>>>,
  thus

  <\equation*>
    L<rprime|'><around*|(|\<theta\><rprime|'>|)>\<assign\><frac|L<around*|(|0|)>|L<rsub|min>>L<around*|(|\<theta\><rsub|\<star\>>,\<theta\><rprime|'>|)>=<big|int>\<mathd\>x
    \<mathd\>y<rprime|'> p<rprime|'><rsub|D><around*|(|x,y<rprime|'>;\<theta\><rsub|\<star\>>|)>
    <around*|(|f<rprime|'><around*|(|x;\<theta\><rprime|'>|)>-y<rprime|'>|)><rsup|2>.
  </equation*>

  Now, <math|L<rprime|'>> is formally the same as <math|L>, even with
  <math|L<rprime|'><around*|(|0|)>=L<around*|(|0|)>>. The <math|f<rprime|'>>
  is equivalent to <math|f> since their spaces of parameters are both
  <math|n>-dimensional. Thus, the only difference is that <math|p<rsub|D>> is
  replaced by <math|p<rprime|'><rsub|D><around*|(|\<ldots\>;\<theta\><rsub|\<star\>>|)>>.

  If the distribution <math|p<rprime|'><rsub|D><around*|(|\<ldots\>;\<theta\><rsub|\<star\>>|)>>
  has exactly the same \Pcomplexity\Q as that of <math|p<rsub|D>>, then we
  shall expect that <math|min<rsub|\<theta\><rprime|'>\<in\>\<bbb-R\><rsup|n>>L<rprime|'><around*|(|\<theta\><rprime|'>|)>=L<rsub|min>>.
  Recalling that <math|p<rprime|'><rsub|D>> characterizes the distribution of
  residual errors of <math|f<around*|(|\<ldots\>;\<theta\><rsub|\<star\>>|)>>,
  this means the residual errors have the same complexity as that of the
  targets. In this case, the minimal loss given by
  <math|f<around*|(|x;\<theta\><rsub|\<star\>>|)>+r
  f<rprime|'><around*|(|x;\<theta\><rprime|'><rsub|\<star\>>|)>>, where
  <math|\<theta\><rprime|'><rsub|\<star\>>\<assign\>argmin<rsub|\<theta\><rprime|'>\<in\>\<bbb-R\><rsup|n>>L<rprime|'><around*|(|\<theta\><rprime|'>|)>>,
  shall be

  <\equation*>
    L<around*|(|\<theta\><rsub|\<star\>>,\<theta\><rprime|'><rsub|\<star\>>|)>=<frac|L<rsub|min>|L<around*|(|0|)>>L<rprime|'><around*|(|\<theta\><rprime|'><rsub|\<star\>>|)>=<frac|L<rsub|min>|L<around*|(|0|)>>
    L<rsub|min>=L<around*|(|0|)> <around*|(|<frac|L<rsub|min>|L<around*|(|0|)>>|)><rsup|2>.
  </equation*>

  \;

  Repeat this process and assume the same self-similarity, the model is
  enlarged from <math|2n>-dimension to <math|4n>-dimension. Replacing
  <math|n> by <math|2n> and <math|L<rsub|min>> by <math|L<around*|(|0|)>
  <around*|(|L<rsub|min>/L<around*|(|0|)>|)><rsup|2>>, the minimal loss comes
  to be <math|L<around*|(|0|)> <around*|(|L<rsub|min>/L<around*|(|0|)>|)><rsup|4>>.<\footnote>
    Indeed, denoting <math|<wide|L|~><rsub|min>\<assign\>L<around*|(|0|)>
    <around*|(|L<rsub|min>/L<around*|(|0|)>|)><rsup|2>>, we have

    <\equation*>
      L<around*|(|0|)> <around*|(|<frac|<wide|L|~><rsub|min>|L<around*|(|0|)>>|)><rsup|2>=L<around*|(|0|)>
      <around*|(|<around*|(|<frac|L<rsub|min>|L<around*|(|0|)>>|)><rsup|2>|)><rsup|2>=L<around*|(|0|)>
      <around*|(|<frac|L<rsub|min>|L<around*|(|0|)>>|)><rsup|4>.
    </equation*>
  </footnote> And extending to <math|8n>-dimension gives the minimal loss
  <math|L<rsub|min><around*|(|L<rsub|min>/L<around*|(|0|)>|)><rsup|8>>. So,
  extending to <math|<around*|(|2<rsup|m>\<times\>n|)>>-dimension gives the
  minimal loss <math|L<around*|(|0|)> <around*|(|L<rsub|min>/L<around*|(|0|)>|)><rsup|2<rsup|m>>>.
  This implies an exponential relation

  <\equation*>
    <around*|(|<text|minimal loss>|)>\<propto\>\<alpha\><rsup|<around*|(|<text|number
    of parameters>|)>>,
  </equation*>

  with <math|\<alpha\>\<assign\>L<rsub|min>/L<around*|(|0|)>>.
</body>

<\initial>
  <\collection>
    <associate|page-medium|paper>
  </collection>
</initial>

<\references>
  <\collection>
    <associate|auto-1|<tuple|1|1>>
    <associate|auto-2|<tuple|1.1|1>>
    <associate|auto-3|<tuple|1.1.1|?>>
    <associate|auto-4|<tuple|1.1.2|?>>
    <associate|auto-5|<tuple|1.1.3|?>>
    <associate|auto-6|<tuple|1.1.4|?>>
    <associate|footnote-1.1|<tuple|1.1|1>>
    <associate|footnote-1.2|<tuple|1.2|?>>
    <associate|footnr-1.1|<tuple|1.1|1>>
    <associate|footnr-1.2|<tuple|1.2|?>>
  </collection>
</references>

<\auxiliary>
  <\collection>
    <\associate|toc>
      <vspace*|1fn><with|font-series|<quote|bold>|math-font-series|<quote|bold>|1<space|2spc>Scaling
      and Power Law> <datoms|<macro|x|<repeat|<arg|x>|<with|font-series|medium|<with|font-size|1|<space|0.2fn>.<space|0.2fn>>>>>|<htab|5mm>>
      <no-break><pageref|auto-1><vspace|0.5fn>

      1.1<space|2spc>Power Law <datoms|<macro|x|<repeat|<arg|x>|<with|font-series|medium|<with|font-size|1|<space|0.2fn>.<space|0.2fn>>>>>|<htab|5mm>>
      <no-break><pageref|auto-2>
    </associate>
  </collection>
</auxiliary>